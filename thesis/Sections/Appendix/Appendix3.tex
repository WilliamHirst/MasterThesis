\section{The implementation of Channel-Out, \ac{SCO} and Maxout}\label{sec:TFImp}
\subsubsection*{Channel-out}
\lstset{style=Python}
\begin{lstlisting}[caption={Python implementation for the custom activation function used to define the channel-out layer.},captionpos=b, label={lst:channel_out}]
def call(self, inputs: tf.Tensor,mask: tf.Tensor = None) -> tf.Tensor:
    # Pass input through weight kernel and adding bias terms.
    inputs = gen_math_ops.MatMul(a=inputs, b=self.kernel)
    inputs = nn_ops.bias_add(inputs, self.bias)

    num_inputs = inputs.shape[0]
    if num_inputs is None:
    num_inputs = -1

    # Reshaping inputs such that they are grouped correctly
    num_competitors = self.units // self.num_groups
    new_shape = [num_inputs, self.num_groups, num_competitors]
    inputs = tf.reshape(inputs, new_shape)

    # Finding maximum activations and setting losers to 0.
    outputs = tf.math.reduce_max(inputs, axis=-1, keepdims=True)
    outputs = tf.where(tf.equal(inputs, outputs), outputs, 0.)
    # Reshaping outputs to original input shape
    outputs = tf.reshape(outputs, [num_inputs, self.units])

    #Count the activate nodes. This variable is used when plotting the activations.
    self.counter = outputs

    return outputs 
\end{lstlisting}

\lstset{style=Python}
\begin{lstlisting}[caption={Python implementation for the custom activation function used to define the \ac{SCO} layer.},captionpos=b, label={lst:channel_out}]
    def call(self, inputs: tf.Tensor, mask: tf.Tensor = None) -> tf.Tensor:
    inputs = gen_math_ops.MatMul(a=inputs, b=self.kernel)
    inputs = nn_ops.bias_add(inputs, self.bias)
    #tf.print(inputs)
    num_inputs = inputs.shape[0]

    if num_inputs is None:
    num_inputs = -1

    shuffle_index = tf.random.shuffle(self.index)
    unshuffle_index = tf.tensor_scatter_nd_update(tensor = self.zeros , 
                                                indices = tf.reshape(shuffle_index, 
                                                                    [inputs.shape[1],1]), 
                                                updates = self.index)
    inputs_s = tf.gather(inputs, shuffle_index, axis = 1)


    # Reshaping inputs such that they are grouped correctly
    num_competitors = self.units // self.num_groups
    new_shape = [num_inputs, self.num_groups, num_competitors]
    inputs_s = tf.reshape(inputs_s, new_shape)

    # Finding maximum activations and setting losers to 0.
    outputs = tf.math.reduce_max(inputs_s, axis=-1, keepdims=True)

    outputs = tf.where(tf.equal(inputs_s, outputs), 1.0, 0.)
    # Reshaping outputs to original input shape
    outputs = tf.reshape(outputs, [num_inputs, self.units])

    outputs = tf.gather(outputs, unshuffle_index, axis = 1) 
    outputs = tf.multiply(inputs, outputs)
    #Count the activate nodes. This variable is used when plotting the activations.
    self.counter = outputs
    return outputs 
\end{lstlisting}
\newpage
\lstset{style=Python}
\begin{lstlisting}[caption={Python implementation for the custom activation function used to define the maxout layer.},captionpos=b, label={lst:max_out}]
def call(self, inputs: tf.Tensor) -> tf.Tensor:
    # Passing input through weight kernel and adding bias terms
    inputs = gen_math_ops.MatMul(a=inputs, b=self.kernel)
    inputs = nn_ops.bias_add(inputs, self.bias)

    num_inputs = inputs.shape[0]
    if num_inputs is None:
        num_inputs = -1
    num_competitors = self.units // self.num_groups
    new_shape = [num_inputs, self.num_groups, num_competitors]

    # Reshaping outputs such that they are grouped correctly
    inputs = tf.reshape(inputs, new_shape)
    # Finding maximum activation in each group
    outputs = tf.math.reduce_max(inputs, axis=-1,keepdims=True)

    counter = tf.where(tf.equal(inputs, outputs), outputs, 0.)
    
    # Reshaping outputs to original input shape
    outputs = tf.reshape(outputs,[num_inputs, self.num_groups])   

    #Count the activate nodes. This variable is used when plotting the activations.
    self.counter = tf.reshape(counter, [num_inputs, self.units])

    return outputs
\end{lstlisting}
\newpage