\newpage
\chapter*{Conclusion $\&$ Outlook}
\addcontentsline{toc}{chapter}{Conclusion $\&$ Outlook} 
In this thesis I have applied a range of \ac{ML} models to the search for chargino-neutralino production resulting in 
a three lepton final state with missing transverse momentum. Two data sets were utilized during the analysis: simulated \ac{MC} data, including 
both the \ac{SM} background and the \ac{BSM} signal, and  measured proton-proton collisions at $\sqrt{s} = 13$ TeV produced at the \ac{LHC} and detected 
at \ac{ATLAS}. The models applied and studied during my analysis were a set of \ac{NN} variants, in addition to a \ac{BDT} which was used to 
create a benchmark for my analysis. The network variants encompassed a diverse array of approaches including an ordinary dense \ac{NN}, ensemble networks 
employing \ac{LWTA} layers and a \ac{PNN}.
\\\newline
The simulated signal set included and studied in the analysis consisted of a set of orthogonal \ac{BSM} variants, specifically different masses for the chargino ($\tilde{\chi}^\pm_1$) 
and neutralino ($\tilde{\chi}_1$). In my analysis I tested two approaches for dealing with a diverse signal set: training one model per variant, and training one model on a larger set 
of variants. Comparing the two, I found that the latter achieved a higher sensitivity as a consequence of a number of factors. By including an assortment of variations of new physics, 
the \ac{ML} models were able to avoid overfitting longer, which allowed for deeper learning. Furthermore, I found that the models were able to exploit overlapping feature distributions between 
the variations, which resulted in more training data for all mass combinations. As a consequence, all further models were trained on a diverse signal set, and large parts of the analysis 
was focused on how each individual method handled this decision.
\\\newline
I studied three variants of \ac{LWTA} layers; channel-out, maxout, and \ac{SCO}. The first two were taken from the paper by Wang et al. \cite{wang_maxout_2013}, and the third layer was introduced
in this present thesis. Each layer reduces the number of nodes during a forward propagation, similarly to the dropout layer, but does so by comparing activation with other nodes in the layer, and 
dropping all but the largest node. To study the implementation and effect of these layers, I constructed a set of figures to visualize the activation and dropping of nodes before and after training. 
When dissecting the figures, I observed that the \ac{LWTA} layers (specifically the maxout layer) were able to build an ensemble of networks by means of pattern-specific paths. In other words, after training 
the model, the data chose different paths through the network dependent on if it was background or signal. Moreover, by comparing two sets of signals with different mass combinations, I found that the model 
was also able to differentiate between different variations of signal. This indicates a strong long-term memory, which allows the model to target a larger set of signal and is an important attribute
in the \ac{LWTA} layers. Additionally to studying the maxout layer, I repeated the analysis for \ac{SCO}. By comparing the activation between the maxout and \ac{SCO} layer, I found that the \ac{SCO} layer 
was able to reduce complex co-adaptation by motivating all nodes to contribute to the output.
\\\newline
The \ac{PNN} applied in the thesis was inspired by the network introduced in the paper by Baldi et al. \cite{PNN}. The purpose of the \ac{PNN} was to motivate the model to differentiate between the mass combination 
in the signal set through including the parameters (choice of chargino and neutralino mass) as a feature. To study the effect of the \ac{PNN} I drew the distribution of a subset of mass combinations, where all the events were given the same parameters.
The idea was that the \ac{PNN} would perform better when events were given the correct parameters, compared to when they were not. By repeating this test, but assigning the data another set of parameters I found that \ac{PNN} did 
perform better when predicting on data which were given the correct parameters, although not by much. For example, compared to when the events were given the wrong parameters, signal events with masses equal to $\{50,250\}_{GeV}$, 
improved effectiveness by $3\%$ when applying a rigid cut on the output of 0.975. This indicates that the \ac{PNN} was able to discriminate between different variations of signal, even if only by 
a small amount.
\\\newline
When studying and comparing the performance of the \ac{ML} models, two different sets of the signal were used, original ($\tilde{\chi}_1\in[0-400]$ GeV and  $\tilde{\chi}_2\in[400-800]$ GeV) and the complete signal grid ($\tilde{\chi}_1\in[0-400]$GeV 
and $\tilde{\chi}_2\in[200-800]$GeV), where the first was a subset of the second. When comparing the achieved sensitivity, or significance, of the models on each mass combination in the original signal set I found that the 
maxout model outperformed all other \ac{LWTA} models, achieving a higher significance in 24 out of the 30 combinations. Although the introduced \ac{SCO} layer did not outperform maxout model, it did achieve a higher significance in 6 
combinations. Most likely, by modifying the \ac{SCO} layer during prediction the performance of the layer would improve.
\\\newline
Four models (not including channel-out and \ac{SCO}) were chosen when comparing performance on the original signal set; ordinary dense \ac{NN}, maxout model, \ac{PNN} and a \ac{BDT} implemented using the default settings of \verb!XGBoost! \cite{XGB}. In the comparison, 
I found that all three network variants were able to outperform the \ac{BDT}, although I believe this to be explained by little attention given towards the tuning of the \ac{BDT}. Out of the three network variants, the maxout model was able to achieve 
the highest significance for most mass combinations (24 out of 30), but mostly in the higher mass range ($\tilde{\chi}_2>600$GeV). In the events with lower masses and higher statistics ($\tilde{\chi}_2<600$GeV), the \ac{PNN} outperformed all others, 
almost doubling the significance achieved by the maxout model. Shared among all models was the fact that they all found processes with high amounts of missing transverse energy difficult to separate from the signal, i.e. $Diboson(lll)$, $t\bar{t}$ and $top\ other$.
\\\newline
Before applying the models to the complete signal grid, I applied a \ac{PCA} to study if it could improve performance. When requiring the conservation of $99.9\%$ of the variation from the original feature set, 5 features were removed.
Training the dense \ac{NN}, maxout model and \ac{PNN} with this new data set, I found it to improve the sensitivity of the two latter models. The comparison was based on the choice to weight the importance of all mass combinations equally.
\\\newline
Finally, in my analysis I compared the performance of my three best performing models (maxout model and \ac{PNN} with a \ac{PCA}, and the dense \ac{NN}) on the complete signal grid. Additionally, I compared the results to the expected exclusion 
limits ($Z>1.64$) set by \ac{ATLAS} in 2021 \cite{atlas_search_2021}. The expected significance achieved by the models were calculated using a flat uncertainty of $20\%$, $10\%$ and $<1\%$. Based on the comparison I found that none of the \ac{ML} models were 
able to extend the limits set by \ac{ATLAS}, with the exception of the \ac{PNN} when utilizing $<1\%$ uncertainty. For an uncertainty of $20\%$, the \ac{PNN} was able to achieve a limit which mirrors \ac{ATLAS} for smaller masses ($\tilde{\chi}_2<250$GeV)
and set a limit past that achieved by the other networks, for both the chargino and neutralino mass. When decreasing the uncertainty, the maxout model and dense \ac{NN} were able to extend the limit past that achieved by the \ac{PNN} for higher masses, 
but never surpassing the limit by \ac{ATLAS}.
\\\newline
From my analysis I found that where the \ac{PNN} exhibits bias towards higher statistic signal, the ordinary dense \ac{NN} and maxout model are able to achieve a more balanced sensitivity. Especially the 
maxout model, with an impressive long-term memory, was able to uphold a strong performance for lower statistics signal and smaller differences in significance ($\Delta Z \approx 10$ when no uncertainty is applied). The effect of the long-term memory 
is further demonstrated in the comparison between the model after training on the original, and the complete signal grid. In the comparison, the maxout model improved its performance on the original set in 17 out of 30 mass combinations compared to the \ac{PNN} which, only improved
2 out of 30. Due to the fact that future analysis will need sensitivity in low statistics regions (high mass), I believe the \ac{LWTA} layers to be interesting candidates for future models in regard to their ability to exploit high statistics combinations while maintaining a 
strong performance on lower statistics signal. 
\\\newline
In conclusion, my results indicate that although none of the \ac{ML} models extended the expected exclusion limit made by the traditional \ac{CC} approach from \ac{ATLAS} in 2021 \cite{atlas_search_2021}, the network variants showed great promise. I believe
that through a more complex analysis of the signal region, both in choice of region and combining the statistics from multiple signal regions, the models would drastically increase in sensitivity. Furthermore, any future \ac{ML} models would greatly benefit from
including multiple overlapping new physics variants, especially through the application of models displaying long-term memory, similarly to the models utilizing the \ac{LWTA} layers in this thesis.
