\newpage
\chapter*{Conclusion $\&$ Outlook}
\addcontentsline{toc}{chapter}{Conclusion $\&$ Outlook} 
In this thesis I have applied a range of \ac{ML} models to the search for chargino-neutralino production resulting in 
a three lepton final state with missing transverse momentum. Two data sets were utilized during the analysis; simulated \ac{MC} data, including 
both the \ac{SM} background and the \ac{BSM} signal, and  measured proton-proton collisions produced at the \ac{LHC} and detected 
at \ac{ATLAS}. The models applied and studied during my analysis were a set of \ac{NN} variants, in addition to a \ac{BDT} which was used to 
create a benchmark for my analysis. The network variants encompassed a diverse array of approaches including an ordinary dense \ac{NN}, ensemble networks employing 
\ac{LWTA} layers and a \ac{PNN}.
\\
The simulated signal set included and studied in the analysis consisted of a set of orthogonal \ac{BSM} variants, specifically different masses for the chargino ($\tilde{\chi}^\pm_1$) 
and neutralino ($\tilde{\chi}_1$). In my analysis I tested two approaches for dealing with a diverse signal set; training one model per variant, and training one model on a larger set 
of variants. Comparing the two, I found the latter to achieve a higher sensitivity as a consequence of a couple of factors. By including an assortment of variations of new physics, 
the \ac{ML} models were able to avoid overfitting longer, which allowed for deeper learning. Furthermore, I found that the models were able to exploit overlapping feature trends between the variations, which 
resulted in more training data for all combinations.  
\\
I studied three variants of \ac{LWTA} layers; channel-out, maxout, and \ac{SCO}. The first two were taken from the paper by Wang et al. \cite{wang_maxout_2013}, and the third layer was introduced
in this present thesis. Each layer, reduces the number of nodes during a forward propagation, similarly to the dropout layer, but does so by comparing activation with other nodes in the layer, and 
dropping all but the largest node. To study the implementation and effect of these layers, I constructed a set of figures which visualize the activation and dropping of nodes, before and after training. 
When dissecting the figures, I observed that the \ac{LWTA} layers (especially the maxout layer) were able to build an ensemble of networks by means of trend specific paths. In other words, after training 
the model the data chose different paths through the network dependent on if it was background or signal. Moreover, by comparing two sets of signal with different mass combinations, I found that the model 
was also able to differentiate between different variations of signal. 
\\

% When studying the performance of the \ac{ML} models, two sets of the signal were used, original and full statistics, where the first was a subset 
% of the second. 