\section{Model Assessment}\label{subsec:Cost}
\subsection{The Rate of True-Positive - ROC Curve}\label{subsec:AUC}
A \ac{ROC} curve is a tool used to measure and visualize a binary classifiers' ability 
to correctly classify data. In figure \ref{fig:ROC} I have plotted an illustration of a 
\ac{ROC} curve. The curve is plotted on an xy-axis where the x-axis represents 
false-positive rates and the y-axis represents true-positive. The different values 
for the curve are the rate of true positives with different thresholds, i.e. 
the value deciding whether an event is 1 or 0, signal or background. If a classifier 
has learned nothing and is simply guessing, the \ac{ROC} curve will be a linear curve 
going from 0 to 1. This line is often drawn in \ac{ROC} curve. The better the 
classifier is, the higher the \ac{ROC} curve will bend towards the upper-left corner of the 
graph. The closer the line is to the diagonal, the worse the classifier. 
\\
A metric often used to measure a classifiers' ability create an output which effectively 
separates two categories, is the \ac{AUC}. The larger the area, the better the separation. 
An ideal classifier which perfectly separates two categories will achieve a \ac{AUC} of 1.
A classifier which simply guesses, will achieve an \ac{AUC} of 0.5. Both this cases assume 
an equal weighting of both signal and background. 
\\
This present section was taken from some of my previous work, and can be found in the following 
rapport \cite{HirstFretteML}.
\begin{figure}
    \centering
    \makebox[0.75\linewidth][c]{%
    \includegraphics[width=0.45\textwidth]{Figures/Illustrations/True-Positive.png}
    }
    \caption{An illustration of a \acs{ROC} curve and how a random, good and perfect classifier would differ.}
    \label{fig:ROC}
\end{figure}
\subsection{Statistical Assessment - Discovery $\&$ Exclusion}\label{subsec:Sensitivity}
The statistical aspect of the analysis will not be of focus in this analysis, nonetheless there are 
some expressions which would be helpful to define. Let us assume that we expect to see no new physics in the collision
data, we can call this hypothesis the b(background)-hypothesis. To expect no new physics, is not the same as expecting no 
deviation. We still assume statistical uncertainties to effect the simulations which will affect the distribution of our 
simulations and lead to fluctuations. If these fluctuations are random, we can assume that the noise can be described by a Gaussian distribution 
(which is a good approximation for large statistics), which has a mean of 0 and a standard deviation equal to $\sqrt{b}$, where b is 
the number of \ac{SM} simulations. Using this assumption, we can state that the higher the deviation, i.e. the further away 
from the mean, the less likely such a deviation is explained by our b-hypothesis. Given a large enough deviation, we can even 
define the hypothesis as rejected with a certain confidence.
\\
We measure the deviation between the observed collision data and the simulated \ac{SM} data in units of significance, Z. 
For a given signal region with $n_{obs}$ events of measured collision data and b simulated \ac{SM} data, we define
the significance of a deviation as
\begin{align}\label{eq:Z1}
Z=\sqrt{2\left[n_{\text {obs }} \ln \frac{n_{\text {obs }}}{b}-n_{\mathrm{obs}}+b\right]} \text { or } 
Z=\sqrt{2\left[(s+b) \ln \left(1+\frac{s}{b}\right)-s\right]}, 
\end{align}
where we have defined the signal as $s = n_{obs} - b$. In the case of large statistics ($b>>s$), we can write Z 
as 
\begin{align}\label{eq:Z}
    Z=\frac{n_{o b s}-b}{\sqrt{b}} = \frac{s}{\sqrt{b}}.
\end{align}
By studying equation \ref{eq:Z} we observe that the significance of a deviation is simply the signal measured in units of standard
deviation of the Gaussian distribution. In physics, one often deems the results a discovery, or the b-hypothesis as rejected, if $Z>5$. 
\\
\begin{figure}[H]
    \centering
    \makebox[0.8\linewidth][c]{%
    \includegraphics[width=0.45\textwidth]{Figures/Illustrations/ConfInt.png}
    }
    \caption{An illustration of a Gaussian distribution and the area under the curve defined by a significance equal to 1.64.}
    \label{fig:ConfInt}
\end{figure}
Before attempting to search for deviations in the real data, we are interested analysing the sensitivity of our method as well 
as calculate the expected number of signal in our signal region. To study the sensitivity of an analysis one looks only at the 
simulated background and signal and performs a study to see how well one can separate background from signal. The measure of the sensitivity 
of our models will be of great focus in this thesis.
\\
In the case we are measuring the sensitivity, we do not have $n_{obs}$. Instead we simply define $s$ as the number of events of the simulated signal 
inside the signal region. This number would be specific for each mass combination and will be a measure for how sensitive 
the model is for that exact combination. We call the significance calulated using the simulated data for \emph{expected significance}. 
In physics, we define a model as sufficiently sensitive, if it is able to achieve a significance of 1.64. This number is chosen based on 
its relation to the b-hypothesis. A significance of 1.64, equals the distance from the mean of the Gaussiwan distribution which sums to an area of 
0.95 (see figure \ref{fig:ConfInt}). There are many ways to inteprate this. A layman's intepration (which is sufficient for this thesis), is 
that given the b-hypothesis is true, there is a $95\%$ probability that we will measure a signal which produces a significance of less than 
1.64. Therefore, there is only a $5\%$ probability, given the b-hypothesis, that we would observe an equal or worse comparison given further experiments. 
For more information on significance and discovery in physics the reader is reffered to the following slides \cite{magnar}.