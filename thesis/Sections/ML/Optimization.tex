\section{Optimization}\label{sec:Opti}
For a general function $g$ dependent one a set of parameters $\boldsymbol \theta = 
\{\theta_0,\theta_1,...,\theta_{N_\theta}\}$, the goal of optimization is to find 
optimal parameters as defined by a predicated goal. In our case we are interested in 
finding the set of parameters corresponding to the minimum value of $g$. Several methods
can be applied to optimization problems, all with their own advantages and disadvantages.
In most methods the use of the gradient of the function, $\grad_\theta (g)$ is involved in 
one way or another. Many of the methods used in this analysis are based on one of the simplest 
optimization methods, the \emph{gradient descent}-method.
\subsection{Gradient Descent}
The gradient descent method aims to obtain the optimal parameters $\tilde{\boldsymbol\theta}$ 
through the application of the derivative of $g$ with respect to $\boldsymbol \theta$. When 
evaluated at a given point in the parameter space $g(\boldsymbol \theta_i)$ the negative of 
the gradient $\grad_\theta (g)$, is used to move $\boldsymbol \theta_i$ closer to $\tilde{\boldsymbol\theta}$.
The negative is because $-\grad_\theta (g)$ corresponds to the direction for which a 
small change $d\boldsymbol\theta$ in the parameter space will result in the biggest 
decrease in the cost function. Finding the minimum value is an iterative process, meaning
the steps in the direction of $-\grad_\theta (g)$ is finite. The size of the step is a
hyperparameter decided by the user and is called the learning rate, $\eta$. The evolution 
from a step i to $i+1$ becomes
\begin{align}
    \boldsymbol{\theta}_{i+1}=\boldsymbol{\theta}_i-\eta \cdot \nabla_\theta g\left(\boldsymbol{\theta}_i\right).
\end{align}
Choosing the $\eta$ can drastically affect the performance of the gradient descent method. 
Too large and one risks "jumping" over the true minimum or simply never allowing for parameters
to reach a high accuracy. Too small and one risks spending computation time beyond reason. 
\subsection{Adam}

\subsection{Cost functions}\label{subsec:Cost}
How we define the performance of a \ac{ML} model is not only important when 
evaluating the model, but is crucial during training. In the case of classification,
it is natural to assume an appropriate metric should involve a comparison between 
the predicted classification and the true classification. The variation of 
performance metrics stems from the diversity of how one quantifies the comparison 
between the two. During training, we define an objective function used to guide 
the model towards optimal tuning. We call this function the \emph{cost function}. 
\\
\subsubsection{Mean Squared Error}\label{subsubsec:MSE}
\ac{MSE}
\subsubsection{Binary Crossentropy}
\begin{align}
    \mathcal{C}\left(Y, T\right) =-\sum_{i=1}^N\left[ \textbf{y}_i \log \left(\textbf{t}_i\right)+\left(1-\textbf{y}_i\right) \log \left(1-\textbf{t}_i\right)\right]
\end{align}
