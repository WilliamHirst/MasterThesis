\section{Optimization}\label{sec:Opti}
For a general function, $\mathcal{C}$, dependent on a set of parameters, $\boldsymbol \theta = 
\{\theta_0,\theta_1,...,\theta_{N_\theta}\}$, the goal of optimization is to find 
optimal parameters as defined by a predicated goal. In the case of \ac{ML}, the parameters are what 
define the model and the function is some metric which defines the error of the prediction. This 
means that in our case, the optimization problem corresponds to
finding the set of parameters corresponding to the minimum value of $\mathcal{C}$. Several methods
can be applied to optimization problems, all with their own advantages and disadvantages.
In most methods the use of the gradient of the function, $\grad_\theta (\mathcal{C})$, is involved in 
one way or another. Many of the methods which were used in the analysis are based on one of the simplest 
optimization methods, the \emph{gradient descent}-method.
\subsection{Cost functions}\label{subsec:Cost}
How we define the performance of an \ac{ML} model is not only important when 
evaluating the model, but is crucial during training. In the case of classification,
it is natural to assume an appropriate metric should involve a comparison between 
the predicted classification and the true classification. The variation of 
performance metrics stems from the diversity of how one quantifies the comparison 
between the two. During training, we define an objective function used to guide 
the model towards optimal tuning. We call this function the \emph{cost function}. 
\\
\subsubsection{Binary Crossentropy}
The \emph{binary crossentropy} function is a very popular cost function for classification problems, as it
can be used to compare the predicted probability distributions to the true probability distributions for a set 
of classification. It is ideal, as it heavily punishes high probabilities (high output) for wrong classification
and heavily rewards high probabilities for correct classifications. For a set of predictions, $Y$, and a set of targets, $T$,
the binary crossentropy function is defined as
\begin{align}
    \mathcal{C}\left(Y, T\right) =-\sum_{i=1}^N\left[ \textbf{y}_i \log \left(\textbf{t}_i\right)+\left(1-\textbf{y}_i\right) \log \left(1-\textbf{t}_i\right)\right].
\end{align}
\subsection{Stochastic Gradient Descent and Mini-Batches}\label{subsec:SGD}
The gradient descent method aims to obtain the optimal parameters of a model
through the application of the derivative of the cost-function, $\mathcal{C}$ with respect to the parameters in the model, 
$\boldsymbol \theta$. When evaluated at a given point in the parameter space, the negative of 
the gradient, $-\grad_\theta (\mathcal{C})$, is used to move closer to the optimal set of parameters.
The negative of the gradient corresponds to the direction for which a 
small change, $d\boldsymbol\theta$, in the parameter space will result in the biggest 
decrease in the cost function. Finding the minimum value is an iterative process, meaning
the steps in the direction of $-\grad_\theta (\mathcal{C})$ is finite. The size of the step is a
hyperparameter decided by the user and is called the learning rate, $\eta$. The evolution 
from a step i to $i+1$ becomes
\begin{align}
    \boldsymbol{\theta}_{i+1}=\boldsymbol{\theta}_i-\eta \cdot \nabla_\theta \mathcal{C}\left(\boldsymbol{\theta}_i\right).
\end{align}
Choosing the $\eta$ can drastically affect the performance of the gradient descent method. 
Having a too large $\eta$ and one risks 'jumping' over the true minimum or simply never allowing for parameters
to reach a high accuracy. Too small $\eta$ and may result in spending computation time beyond reason as well as being sensitive
to local minima. 
\\
For a given data set of size $N$, we find the gradient ($\grad_\theta (\mathcal{C})$) through the sum of the gradient for each data point, 
$\textbf{x}_i$, as
\begin{align}\label{eq:dC}
    \grad_\theta (\mathcal{C}) = \sum_{i=0}^N \grad_\theta (\mathcal{C}_i(\textbf{x}_i)).
\end{align}
Note, that in the equation above, each data point is given an equal weighting in the sum. This does not necessarily have to be the case. 
In the scenario that the points in the data set should be prioritized differently during training, a simple weight, $w_i$, can be multiplied in the sum of 
equation \ref{eq:dC}. We call these weights, \emph{sample-weights}.
\\
The calculation of the gradient over numerous data points can be time-consuming. An alternative approach which both reduces time and 
introduces randomness, is the application of \emph{mini-batches}. Instead of summing the gradient over the full data set, one randomly samples 
a predetermined number of points, $B$, creating a subset of the data which is used to update the parameters. The points sampled to create the subset, can be 
sampled both with or without replacement\footnote{In this analysis the data is sampled with replacement.}, meaning allowing the same data points to be sampled 
several times or not. An epoch is completed after $N/B$ iterations of updating the parameters. The added stochastic element reduces the risk of getting stuck 
in local minima. Gradient descent with randomly sampled mini-batches is called \ac{SGD}.
\subsection{Memory, Adaptive Learning and ADAM}\label{subsec:ADAM}
Although \ac{SGD} can be highly effective, it tends to be prone to oscillations between two points in the parameter space. In other words jumping back and forth between 
the same two sets of parameters, without ever converging closer to the minima. To alleviate this issue, we introduce momentum. The momentum aims to act as a form of 
memory, conserving some momentum from the previous update. By defining the new step between $\theta_{i+1}$ and $\theta_{i}$ as 
\begin{align}
    \textbf{v}_i = \eta \cdot \nabla_\theta \mathcal{C}\left(\boldsymbol{\theta}_i\right),
\end{align}
we can implement momentum to the algorithm as 
\begin{align}
    \textbf{v}_i = \textbf{v}_{i-1} \gamma +\eta \cdot \nabla_\theta \mathcal{C}\left(\boldsymbol{\theta}_i\right),
\end{align}
making our new parameter iteration equal to
\begin{align}
    \boldsymbol{\theta}_{i+1}=\boldsymbol{\theta}_i-\textbf{v}_i,
\end{align}
where $\gamma \in [0,1]$ is a parameter which defines the size of the momentum.
\\
An additional flaw in the \ac{SGD} optimization algorithm, is that it treats all regions in the feature space equally. Ideally, we would want the algorithm to focus 
on the most relevant areas (steep regions), while quickly moving past less relevant areas (flat regions). A solution to this is the introduction of an \emph{adaptive learning rate}.
By updating the learning rate based on the surface of the feature space, one can assign a prioritization in the feature space which optimizes the search for optimal parameters.
\\
The most intuitive way to prioritize steep areas, is by introducing the second derivative of $\mathcal{C}$, but this can be quite time-consuming. The \ac{ADAM} optimizer aims to 
create an approximation of the second derivative, through the use of momentum and an adaptive learning rate. The \ac{ADAM} optimizer introduces a set of new parameters, and includes both the 
linear and squared term of the derivative of $\mathcal{C}$ (i.e. $\nabla_\theta \mathcal{C}$ and $(\nabla_\theta \mathcal{C})^2$) with memory parameters for both ($\beta_1$ and $\beta_2$ respectively). Additionally, the \ac{ADAM} optimizer holds a memory of the average of the 
gradient and squared gradient, which is used to create faster convergence. For a thorough description of the \ac{ADAM}, the reader is referred to the paper by Kingma et al.  \cite{kingma_adam_2017}.
