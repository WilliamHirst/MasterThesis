\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction} 
The \ac{SM} is one of the most successful scientific theory ever
created. It accurately explains the interactions of leptons and quarks as well as the force
carrying particles which mediate said interactions. The model is a result of over a century of work
demanding the contributions of great minds like Paul Dirac, Erwin Schrodinger and Richard Feynman.
In 2012 the SM achieved one of its crowning achievements when we discovered the Higgs boson \cite{Aad_2012}. 
Much of the accolade was rightfully given to the theoretical work on the \ac{SM}, but another aspect of the discovery 
was equally important. Data analysis was and is a crucial part of any new discovery in physics. One of the most important 
and exiting tools is \ac{ML}.\newline
\\
In spite of the success of the \ac{SM}, there are still questions we are unable to answer.
The \ac{SM} is yet to merge the physics of the micro and macro or even accommodate the particles that make
up most of the universe. Theoretical physicists are constantly at work trying to accommodate these aspects of the universe,
introducing extensions of the \ac{SM} like \ac{SUSY} \cite{SUSY} or \emph{String Theory} \cite{cole_probing_2021}. For each new theory, 
the demands for data grows. Before the pandemic, it was projected an increase from $139fb^{-1}$ to approximately $500fb^{-1}$, in the span from 
2020 to 2024, then further doubling in the 7 years following\footnote{The projections were taken from the graph created by \ac{ATLAS} 
\href{https://lhc-commissioning.web.cern.ch/schedule/images/LHC-nominal-lumi-projection.png}{https://lhc-commissioning.web.cern.ch/schedule/images/LHC-nominal-lumi-projection.png}
(Accessed: 21.04.2023).}. With the data sets in particle physics progressivly growing, so do the demand for numerical algorithms and frameworks to analyze them.
One of the most exiting tools which is and will play an important role in upholding this demand, is \ac{ML}.\newline
\\
\acf{ML} is rapidly becoming an overwhelming presence in many scientific fields.
In areas ranging from cancer research to stock-trading, machine learning is being applied to problems
once thought as impossible to solve. Particle physics, like many other fields is no exception. Jet flavor classification \cite{Guest_2016}, 
separating jets from gluons \cite{PhysRevD.44.2025} or using \ac{ML} to create efficient \ac{SR} are just some examples
where \ac{ML} is a vital tool. The traditional approach for ML in high-energy physics is the use of supervised
\ac{DNN}. \ac{DNN}'s are famous for their versatility which is partly due to their diversity in architecture and 
application. In later years \ac{BDT} have become more and more popular, especially after the release of XGBoost 
in 2014. The performance of XGBoost matches that of the \ac{DNN} in many cases, as well as having the advantage 
of being both stable and easy to use. \newline
\\
This present thesis will study \ac{ML} as it is applied to the search for new physics in proton-proton collisions produced by the \ac{LHC} and collected by 
the \ac{ATLAS} detector. Specifically, I will study different \ac{ML} models as they search for chargino-neutralino pair production in final states 
with three leptons and missing transverse momentum. I explored a range of \ac{ML} models, such as ordinary dense \acl{NN}, \acl{PNN} \cite{PNN},
ensemble models including layers introduced in the paper \cite{wang_maxout_2013} and \acl{BDT}. Additionally, I introduced a new layer, \acl{SCO}, which 
resembles the channel-out layer described in the aforementioned paper. Each model was studied and tested in their ability to achieve sensitivity in a diverse 
data set, including events representing different variations of new physics. Particularly, variations in choice of mass for the chargino and neutralino.
\subsubsection*{Outline of the Thesis}
This thesis is divided into 4 chapters; the first two introducing relevant theory and background for the analysis, the third presenting 
details on the implementation of the analysis, and the fourth presenting and discussing the results. At the end of the analysis I will summarize 
the findings in my analysis in the \emph{Conclusion $\&$ Outlook} section, as well as include some additional figures and tables 
in the appendix. 
\\
The \emph{first} chapter will give an introduction of the \ac{SM} as well as discuss the new physics I will be searching for. This chapter will 
introduce relevant phenomenology surrounding particle physics, give a quick description of proton-proton collisions at the \ac{LHC} and discuss 
the physics behind the data set utilized in the analysis. 
\\
The \emph{second} chapter cover the necessary background in regard to \ac{ML} and data analysis in general. This chapter will introduce relevant phenomenology
surrounding data analysis topics such as optimization, regularization and hyperparameters. It will explain the algorithms underlying the \ac{NN} and 
\ac{BDT}'s, as well as introduce methods to elevate the aforementioned methods. In the final parts of the chapter, I will discuss how \ac{ML} relates to 
a \ac{BSM} search and how one assess the results.
\\
The \emph{third} chapter dives into the details of the implementations of the analysis. This will include discussing the relevant frameworks, data formats and 
other tools, as well as diving a little further into the data set. Additionally, this chapter will present the preselection cuts and other preprocessing steps
used to generate the final data sets, both simulated and measured collision data and present the comparison between the two. Finally, this chapter will present the 
models I studied in the analysis, displaying the architectures, and explaining the general strategy utilized for training and validating the models.
\\
In the \emph{fourth} and final chapter, I present and discuss the results from the analysis. In the first 4 sections, I study four different categorize of \ac{ML} modes in how they 
perform on a subset of the data, as well as different attributes of each model. The 5'th section compares the performance of the 4 aforementioned models, then compare 
their results with and without a \ac{PCA} in the 6'th section. Finally, I compare the three best models I found when testing on a subset of the data on how well they perform 
on the full statistics, then compare all three to a previous analysis made by ATLAS.



