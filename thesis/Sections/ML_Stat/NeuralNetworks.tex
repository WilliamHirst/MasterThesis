\subsection{Neural Networks}\label{sec:NN}
\ac{NN} have been an idea for more than 80 years and are today one of the 
most popular and successful \ac{ML} methods. The key to its popularity stems from
its versatility, achieving high performance in a large range of both regression 
and classification problems. One of the defining qualities in a \ac{NN} is the 
possibility of diverse architecture, meaning that a there are many
categorize of \ac{NN}, where each category has an even deeper selection of
networks. Categories ranging from \ac{CNN}, \ac{RNN} to simple \ac{FFNN}, where each
category is specified for their own set of problems. In this section I will introduce 
some fundamental definitions in regard to \ac{NN}, as well go through the underlying 
algorithm of the back- and forward propagation.

\subsubsection*{General structure}
There are often drawn comparisons between the structure of the neural network, 
and the way the human mind operate, hence neural. Similarly to the human mind, a \ac{NN} is 
composed of different neurons communicating information backwards and forwards in different 
structures. In the case of a neural network we call these structures layers. All layers
are composed of a specified number of neurons. A \ac{NN} has three types of layers;
input-layer, hidden-layer and output-layer. There is only one input layer, and it has
the same number of nodes equal to the number of features for each data point. 
There can be an arbitrary number of hidden layers, with each hidden-layer containing
an arbitrary number of nodes. Finally, the \ac{NN} has an output layer. The output-layer
contains a number of nodes equal to the number of features for the target.
\\

\subsubsection{Feeding Forward} 
The forward mechanism is the main mechanism behind the usage of the \ac{NN}.
For a single data point the data flow becomes as outlined in the following.
\begin{enumerate}
    \item The input nodes receive the input data for each feature. 
    \item Each input node sends the data value into each node of the following hidden layer with a scaling according to the associated weight of every single connection. 
    \item Each node in the hidden layer sums up the weighted contributions from the input layer and adds a bias value associated to that given node. We denote this raw node output by $z$.
    \item The unactivated value $z$ is immediately sent through an activation function $\sigma$ associated with the layer to produce the activated value $a = \sigma(z)$.
    \item Each node in the hidden layer then forwards the activated value into the next layer using the same procedure. Notice that the number of nodes in the hidden layers no longer corresponds to any given features and one can only speculate on how the complex network creates its own sub-features and interprets the underlying correlations in the data. 
    \item Finally the stream of forwarded data enters the output layer where the activation values on each node corresponds to the network predictions for each category. 
\end{enumerate}
The feed forward step to obtain the activation value on a given layer $l\ne 1$ (not the input layer) is given as
\begin{align}
    z_j^l = \sum_{i = 1}^{n_{l-1}} w_{ij}^l a_i^{l-1} + b_j^l, \quad a_j^l = \sigma^l(z_j^l)
    \label{eq:feed_forward_sum}
\end{align}
where $z_j^l$ is the input value to node $j$ in layer $l$, $w_{ij}^l$ is the weight connecting node $i$ in layer $l-1$ to node $j$ in layer $l$, $a_i^{l-1}$ is the activation value from node $i$ in layer $l-1$ and $b_j^l$ is the bias associated with node $j$ in layer $l$. We can obtain a matrix-variant of equation \ref{eq:feed_forward_sum} by defining
\begin{align*}
     W^l &= 
        \renewcommand\arraystretch{1.3}
        \begin{bmatrix}
          w_{11}^l & w_{21}^l & \hdots & w_{n_{l-1}1}^l \\
          w_{12}^l & w_{22}^l & \hdots & w_{n_{l-1}2}^l \\
          \vdots & \vdots & & \vdots \\
          w_{1 n_{l}}^l & w_{2 n_{l}}^l & \hdots & w_{n_l n_{l-1}}^l \\
        \end{bmatrix}, \\ \\
    (W^l)^T &= 
    \renewcommand\arraystretch{1.3}
    \begin{bmatrix}
      w_{11}^l & w_{12}^l  & \hdots & w_{1n_l}^l \\ 
      w_{21}^l & w_{22}^l  & \hdots & w_{2n_l}^l \\
      \vdots & \vdots & & \vdots \\
      w_{n_{l-1}1}^l & w_{n_{l-1}2}^l  & \hdots  & w_{n_{l-1}n_l}^l 
    \end{bmatrix}, \\ \\
    A^l &= 
    \renewcommand\arraystretch{1.4}
    \begin{bmatrix}
      a_1^l \\
      a_2^l \\
      \vdots \\
      a_{n_l}^l \\
    \end{bmatrix} , \qquad 
    B^l = 
    \begin{bmatrix}
      b_1^l \\
      b_2^l \\
      \vdots \\
      b_{n_l}^l \\
    \end{bmatrix}, \qquad
      Z^l = 
    \begin{bmatrix}
      z_1^l \\
      z_2^l \\
      \vdots \\
      z_{n_l}^l \\
    \end{bmatrix}, 
\end{align*}
such that we get 
\begin{align}
    Z^l = (W^l)^T A^{l-1} + B^l, \qquad A^l = \sigma(Z^l).
    \label{eq:feed_forard_matrix}
\end{align}
The complete feed forward algorithm is simply done by using equation \ref{eq:feed_forard_matrix}
for layer $l = 2, \hdots, L$, for which we produce the \ac{NN} prediction $\vec{y}$.

\subsubsection{Back Propagation} \label{subsubsec: Bp}
%We assume now that we have a network with initialized weights and biases 
The back propagation algorithm is the essential algorithm behind the training of the neural network. This serves the purpose of tuning the weights and biases according to a given cost function. A common choice for regression type problems is the mean squared error
\begin{align}\label{eq:cost_mse}
    C(\vec{y}) = ||\vec{y}-\vec{t}||_2^2 = \frac{1}{N}\sum_{i=1}^N (y_i - t_i)^2,
\end{align}
where $N$ is the number of data points. However, one can choose from a wide selection of cost functions in general. For classification cases it is normal to use Cross entropy as the cost function. In that case it is defined as 

\begin{align}\label{eq:cross_entropy}
   C(\vec{y}) =  -\sum_{i=1}^n \ \Big[ &y_i\log(t_i) + (1-y_i)\log ( 1 - t_i) \Big].
\end{align}
By calculating the gradient gradient $\nabla_{w,b}C$ with respect to the weights and biases, we can use gradient descent (see section \ref{sec:GD}) to minimize the cost function. Thus we need to evaluate $\partial C/\partial w_{ij}^l$ and $\partial C/\partial b_j^l$. We begin by looking at the last layer $l = L$. By using the chain rule we get
\begin{align*}
     \frac{\partial C}{\partial w_{ij}^L} = \frac{\partial C}{\partial a_j^L}\frac{\partial a_j^L}{\partial z_j^L} \frac{\partial z_j^L}{\partial w_{ij}^L}.
\end{align*}
We remember that 
\begin{align*}
    &a_j^L = \sigma(z_j^L),& &z_j^L = \sum_{i = 1}^{n_{L-1}} w_{ij}^L a_i^{L-1} + b_j^L,&
\end{align*}
such that we get 
\begin{align*}
     \frac{\partial C}{\partial w_{ij}^L} = \frac{\partial C}{\partial a_j^L} \sigma'( z_j^L) a_i^{L-1}.
\end{align*}
Notice that the remaining derivatives can easily be calculated when deciding on a specific  cost and activation function. For the bias we simply get 
\begin{align*}
     \frac{\partial C}{\partial b_j^L} = \frac{\partial C}{\partial a_j^L}\frac{\partial a_j^L}{\partial z_j^L} \frac{\partial z_j^L}{\partial b_j^L} = \frac{\partial C}{\partial a_j^L} \sigma'(z_j^L) \cdot 1.  
\end{align*}
We use this to motivate the definition of the local gradient
\begin{align*}
    \delta_j^l \equiv \frac{\partial C}{\partial z_j^l},
\end{align*}
such that 
\begin{align*}
    \delta_j^L = \frac{\partial C}{\partial z_j^L} =  \frac{\partial C}{\partial a_j^L}\frac{\partial a_j^L}{\partial z_j^L}  =  \frac{\partial C}{\partial a_j^L}\sigma'( z_j^L).
\end{align*}
This yields the more compact expressions
\begin{align*}
    &\frac{\partial C}{\partial w_{ij}^L} = \delta_j^L a_i^{L-1},& & \frac{\partial C}{\partial b_j^L} = \delta_j^L.&
\end{align*}
The local gradient $\delta_j^l$ is also commonly called the "error" since it reflects how big of an influence the j\textsuperscript{th} node in layer $l$ have on the change of the cost function. We let $\boldsymbol{\delta}^l$ denote the vector containing all the local gradients associated with layer $l$ and we can write it out as a matrix equation for the last layer
\begin{align*}
    \boldsymbol{\delta}^L = \nabla_a C \odot \frac{\partial \sigma}{\partial z^L}, \quad  \nabla_a C = \left[\frac{\partial C}{\partial a_1^L}, \frac{\partial C}{\partial a_2^L}, \hdots, \frac{\partial C}{\partial a_{n_L}^L}\right]^T,
\end{align*}
where $\odot$ is the Hadamard product (element-wise product). We can then define the gradient $\delta_j^l$ for the j\textsuperscript{th} node on a general layer $l$ in terms of $\delta_k^{l+1}$ for the k\textsuperscript{th} node on the following layer $l+1$ by using the chain rule
\begin{align}
    \delta_j^l &\equiv \frac{\partial C}{\partial z_j^l} \nonumber \\
    &= \sum_k \frac{\partial C}{\partial z_k^{l+1}}\frac{\partial z_k^{l+1}}{\partial z_j^l} \nonumber\\
    &= \sum_k \frac{\partial z_k^{l+1}}{\partial z_j^l} \delta_k^{l+1}.
    \label{eq:error_j^l}
\end{align}
We remember 
\begin{align*}
    z_k^{l+1} = \sum_{j = 1}^{n_l} w_{jk}^{l+1} a_j^l + b_k^{l+1} = \sum_{j = 1}^{n_l} w_{jk}^{l+1} \sigma(z_j^l) + b_k^{l+1},
\end{align*}
and by taking the derivative we obtain
\begin{align}
    \frac{\partial z_k^{l+1}}{\partial z_j^l} =  w_{jk}^{l+1} \sigma'(z_j^l).
    \label{eq:der_z_k^l+1}
\end{align}
We substitute back \ref{eq:der_z_k^l+1} into \ref{eq:error_j^l} to find
\begin{align*}
    \delta_j^l = \sum_k  \delta_k^{l+1}  w_{ij}^{l+1}\sigma'(z_j^l).
\end{align*}

The complete back propagation algorithm then becomes 
\begin{itemize}
    \item Compute
    \begin{align*}
    \delta_j^L =  \frac{\partial C}{\partial a_j^L}\sigma'( z_j^L). 
    \end{align*}
    \item For $l = L-1, L-2, \hdots, 1$   compute:
    \begin{align*}
        \delta_j^l = \sum_k  \delta_k^{l+1}  w_{jk}^{l+1}\sigma'(z_j^l).
    \end{align*}
    \item For all layers $l$ update weights and biases: 
    \begin{align*}
        w_{ij}^l &\leftarrow w_{ij}^l - \eta \delta_j^l a_i^{l-1}, \\
        b_j^l &\leftarrow b_j^l - \eta \delta_j^l,
    \end{align*}
    where $\eta$ is the learning rate.
\end{itemize}
In order to minimize the risk of overfitting it is common to include a L2 regularization by adding the L2 norm to the cost function as $\lambda||w||_2^2$ and $\lambda||b||_2^2$.
This result in the modified expressions for the update 
\begin{equation*}
    \begin{aligned}
        w_{ij}^l &\leftarrow w_{ij}^l - \eta ( \delta_j^l a_i^{l-1} + \lambda w_{ij}^l ), \\
            b_j^l &\leftarrow b_j^l - \eta (\delta_j^l + \lambda b_{j}^l).
     \end{aligned}
\end{equation*}
