\section{The Machine Learning Models}
\subsection{Model Selection}
In this analysis I have chosen to compare 4 different 'types' of \ac{ML} models, ordinary dense \acf{NN}\footnote{Note 
that for the remaining part of this thesis, I will refer to this model as a 'dense \ac{NN}', 'ordinary dense \ac{NN}' 
and in some plots, just \ac{NN}.}, \acf{PNN}, ensembles (networks utilizing the channel-out, \ac{SCO} or maxout layer) 
and \acf{BDT}. The first three methods are all variations of \ac{NN}. I have deliberately 
chosen to focus on \ac{NN} given that there is far more freedom in the design of the architecture
of a \ac{NN} compared to \verb!XGBoost!. Additionally, this was motivated by the selfish reason
being that I found the networks more interesting to study and dissect. Therefore, most of the 
analysis, comparisons and discussions are focused on the \ac{NN}s, while \verb!XGBoost! is included 
as a loose benchmark. 
\\
The choice of the three network architectures is motivated by a wish to compare three relatively different approaches.
Especially in how they perform with a diverse data set. The ensembles, or \ac{LWTA} models (channel-out, \ac{SCO} and maxout), apply pattern-specific paths, 
the \ac{PNN} includes parameters in the feature set, and the dense \ac{NN} relies on a deep set of weights and biases to uphold long-term memory.
I would assume that the optimal architecture would be a network which combines elements from each, but for the purpose of discussion
and research I have chosen to keep them separate. 
\\
All \ac{NN} variants implemented in this analysis were created, trained and deployed using the \verb!TensorFlow! framework \cite{tensorflow}.
This choice was rooted in several factors, for example that \verb!TensorFlow! has an intuitive \ac{API}, it has very effective
\ac{HPC} attributes, and it includes a large diversity in functionality which allows for experimentation.
\subsection{Model Architecture}\label{subsec:arch}
When choosing a network architecture, there are several ways to proceed. One way is to apply a grid search.
A grid search is simply defining a grid of parameters to test, then running through all combinations and 
choosing the highest performer. With a sufficient amount of tests, a grid search should converge towards 
an optimal architecture. Grid searches are very common and there are a large range of very complex varieties \cite{GS}.
For my analysis I chose not to perform a grid search, for several reasons. The first being interpretability.
Understanding a \ac{NN} is already hard, allowing for complex and unique architectures would only add another layer
of complication. The second is the size of the data set. The larger the data set, the more data 
would be needed to adequately perform tests for each combination of parameters. Not only is this time-consuming,
but trying to fix this issue could lead to poor performance. The third and most important reason is that 
I wanted to experiment with the architectures. By manually tuning the parameters I was able to achieve a far 
better understanding of the final architecture. In the following sections I will describe the architecture of all the 
variations of networks and the \ac{BDT} used in this analysis.
\begin{figure}
    \makebox[0.9\linewidth][c]{%
    \centering
    \begin{subfigure}{1.1\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Illustrations/architecture.png}
    \end{subfigure}
    }
    \caption{A visual summary of the network architectures used in this analysis, for the ordinary dense \acs{NN} (top), the \acs{PNN} (middle)
    and the maxout model (bottom).}
    \label{fig:arch}
\end{figure}
\subsection*{Dense Networks - Ordinary and PNN}\label{subsec:PNNArch}
The purpose of the dense \ac{NN} is to compare the more complex networks to what is usually considered an ordinary dense \ac{NN}.
The general structure of the network is summarized in figure \ref{fig:arch}, with the network label \ac{NN}. The figure shows a dense \ac{NN} with 
three hidden layers, all with 600 nodes each.  As is the default setting of the dense layer implemented in \verb!TensorFlow!, the weights and biases are initialized 
using the so-called \emph{Glorot Uniform Initializer} (see the article by Glorot \cite{glorot_understanding_2010} for more information). All hidden layers 
utilize the $LeakyReLU$ activation (see section \ref{subsec:activation}) with $\alpha$ = 0.01, and the output layer utilizes the sigmoid function.  
The \ac{NN}, similarly to all network variants in this analysis, utilizes the \ac{ADAM} optimizer (see section \ref{subsec:ADAM}) implemented in 
\verb!TensorFlow!\footnote{See \url{https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam}
(Accessed 24.04.2023) for the full default parameters.} with the default settings, where the cost function is chosen to be the binary crossentropy function described
in section \ref{subsec:Cost}. 
\\
The \ac{PNN} architecture is included to represent the model proposed in the article by Baldi et al. \cite{PNN}.
The architecture is illustrated in figure \ref{fig:arch}, with the label \ac{PNN}. The figure shows a practically identical 
structure to the dense \ac{NN}, with the only difference being in the input layer. As was discussed in section \ref{subsec:PNN},
the \ac{PNN} includes free parameters of the signal\footnote{In our case, the masses of the chargino and neutralino.} alongside the features
in the input layer.
\subsection*{Maxout, Channel-Out and SCO}\label{subsec:LWTAARch}
The ensemble models are slightly more complex than both the dense \ac{NN} and \ac{PNN} in terms of architecture. To limit the complexity for comparison reasons
I choose to build an identical architecture for the maxout, channel-out and \ac{SCO} model, with the only difference being which of the three layers is used in the model.  
In figure \ref{fig:arch} I have illustrated the maxout model architecture, with the label MaxOut. The figure shows a network with six hidden layers, 
three maxout and three dropout layers applying a dropout rate of $0.15$. The network alternates between dropout and maxout, starting with dropout and finishing with maxout. The maxout layers 
have 600 nodes each which is reduced to the 200 nodes with the largest activation in their respective units. Each dropout layer has a dropout 
rate of 0.15. The Channel-out and \ac{SCO} models have the same architecture as the maxout, but replacing the maxout layer with each of the two, respectively.  

\subsection*{BDT}\label{subsec:XGBoost}
The main motivation to apply a \ac{BDT} is simply to benchmark my analysis, and therefore not a lot of effort has been put into the design of the 
architecture. Furthermore, \ac{BDT}s have far fewer hyperparameters. As a consequence, the default parameters\footnote{See \url{https://xgboost.readthedocs.io/en/stable/parameter.html}
for a complete overview of default parameters.} of the \verb!XGBoost! model have been used. 
The main parameters of the model are summarized as the following:
\begin{itemize}
    \item $\eta$ (learning-rate) = 0.3
    \item Max depth = 6
    \item Maximum number of trees = 100
    \item Objective = 'binary:logistic'
\end{itemize}
The objective parameter refers to the learning task of the model, where \emph{'binary:logistic'} specifies logistic regression for binary classification.

\subsection{Creating Custom Layers}\label{subsec:CustomLayer}
The field of \ac{ML} is one of the most dynamic and fastest growing fields of research
today. This means that regardless, of the brave attempt made by voluntary contributors and many large tech companies,
there will always be new and exciting \ac{ML} tools and algorithm not yet implemented in their library. This was 
certainly the case in this thesis. For several of the non-dense layers I was forced
to dive into the world of \ac{ML} development and create my own implementation. 
\\
All \ac{LWTA} methods described in section \ref{subsec:LWTA}, channel-out, 
\ac{SCO} and maxout, resemble a layer already implemented by \verb!TensorFlow!. This layer is called 
MaxPooling1D\footnote{For more information on MaxPoolind1D, the reader is referred to the documentation 
(\url{https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool1D},
accessed 24.03.2023).}. But, to allow for experimentation I decided to not use \verb!MaxPooling1D!, but instead 
implement my own custom layers. All custom layers were implemented by creating new functions inside 
\verb!TensorFlow!'s dense layer, which are called when the network performs a forward pass. In the following sections 
I will describe the algorithms underlying the implementation of each layer. For the actual \verb!python!/\verb!TensorFlow! implementation,
see appendix \ref{sec:TFImp}.
\subsubsection*{Channel-Out}
In algorithm \ref{alg:channel-out}, I summarized the implementation of channel-out used in this analysis. For each 
input which is to be passed through a forward-pass, the function first passes it through the weights and biases 
as described in section \ref{subsec:FP}. This is true for all the other layers as well. In line 7 I reshape the input 
to add a dimension which creates the units. In line 10 I create a new variable, $Output$, by reducing each unit to the 
largest activated node. Further, I set each node which is not equal to the largest activation to 0. Finally, I reshape 
the output to the original shape and return the output.  
\begin{algorithm}
    \caption{The pseudocode for implementing the channel-out layer in TensorFlow}\label{alg:channel-out}
    \begin{algorithmic}[1]
    \State def \textbf{Channel-out}(Input): 
    \State \ \ \ \ $\%$ Pass input through weight kernel and adding bias terms
    \State \ \ \ \ $Input \gets Input \times Weights$
    \State \ \ \ \ $Input \gets Input + Bias$
    \\
    \State \ \ \ \ $\%$ Reshape input into units
    \State \ \ \ \ $Input \gets \textbf{Reshape}(Input,(Nr\ Units,\ Size \ of \ Units))$
    \\
    \State \ \ \ \ $\%$ Reduce input to the largest activation in each unit
    \State \ \ \ \ $Output \gets \textbf{Max}(Input)$
    \\
    \State \ \ \ \ $\%$ Set original activation where activation is largest and 0 where it's not
    \State \ \ \ \ $Output \gets \textbf{Where}(Input == Output, Input,0)$
    \\
    \State \ \ \ \ $\%$ Reshape to original size
    \State \ \ \ \ $Output \gets \textbf{Reshape}(Output,(Input's \ Original \ Shape))$
    \State \ \ \ \ $\textbf{return}\ Output$
    \end{algorithmic}
\end{algorithm}
\subsubsection*{SCO}
In section \ref{subsubsec:stochchannelout} I described how \ac{SCO} is an extension of channel-out. This is certainly
also the case for the algorithms explaining the implementation. In algorithm \ref{alg:SCO} I have described the
implementation of the \ac{SCO} layer. Similar to channel-out, the algorithm begins by 
passing input through weights and biases. Then, contrary to channel-out, the nodes of the input are shuffled. This 
is to ensure that when the input is reshaped (in line 10), the units are made differently for each pass through 
the function. Again the maximum activation is found to create a new variable, this time called $OutputShuffle$. 
In line 16 $InputShuflle$ and $OutputShuffle$ are compared such that where $InputShuflle$ is equal to the largest 
activation, the value is set to 1 and otherwise set to 0. $OutputShuffle$ is then unshuffled, and reshaped to the 
original shape. Finally, the input which has been passed through the weights and biases, is multiplied with 
$Output$ and returned.
\begin{algorithm}
    \caption{The pseudocode for implementing the SCO layer in TensorFlow}\label{alg:SCO}
    \begin{algorithmic}[1]
    \State def \textbf{SCO}(Input): 
    \State \ \ \ \ $\%$ Pass input through weight kernel and adding bias terms
    \State \ \ \ \ $Input \gets Input \times Weights$
    \State \ \ \ \ $Input \gets Input + Bias$
    \\
    \State \ \ \ \ $\%$ Shuffle all the values
    \State \ \ \ \ $InputShuflle \gets \textbf{Shuffle}(Input)$
    \\
    \State \ \ \ \ $\%$ Reshape input into units
    \State \ \ \ \ $InputShuflle \gets \textbf{Reshape}(InputShuflle,(Nr\ Units,\ Size \ of \ Units))$
    \\
    \State \ \ \ \ $\%$ Reduce input to the largest activation in each unit
    \State \ \ \ \ $OutputShuffle \gets \textbf{Max}(InputShuflle)$
    \\
    \State \ \ \ \ $\%$ Set 1 where activation is largest and 0 where not
    \State \ \ \ \ $OutputShuffle \gets \textbf{Where}(InputShuflle == OutputShuffle, 1,0)$
    \\
    \State \ \ \ \ $\%$ Un-shuffle all the values
    \State \ \ \ \ $Output \gets \textbf{UnShuffle}(OutputShuffle)$
    \\
    \State \ \ \ \ $\%$ Reshape to original size
    \State \ \ \ \ $Output \gets \textbf{Reshape}(Output,(Input's \ Original \ Shape))$
    \\
    \State \ \ \ \ $\%$ Multiply input with output to set all input that are not the largest, to zero
    \State \ \ \ \ $NewOutput \gets Input \times Output$
    \\
    \State \ \ \ \ $\textbf{return}\ NewOutput$
    \end{algorithmic}
\end{algorithm}
\subsubsection*{Max-Out}
In algorithm \ref{alg:maxout} I have summarized the logic behind the implementation of the maxout layer.
Of the three layers (channel-out, \ac{SCO} and maxout), maxout was the simplest to implement. After passing 
the input through the weights and biases and reshaping it to form the units, the input is reduced to only include 
the largest activation in each layer. Finally, the output is reshaped to the size equal to the number of units.
\begin{algorithm}
    \caption{The pseudocode for implementing the maxout layer in TensorFlow}\label{alg:maxout}
    \begin{algorithmic}[1]
    \State def \textbf{MaxOut}(Input): 
    \State \ \ \ \ $\%$ Pass input through weight kernel and adding bias terms
    \State \ \ \ \ $Input \gets Input \times Weights$
    \State \ \ \ \ $Input \gets Input + Bias$
    \\
    \State \ \ \ \ $\%$ Reshape input into units
    \State \ \ \ \ $Input \gets \textbf{Reshape}(Input,(Nr\ Units,\ Size \ of \ Units))$
    \\
    \State \ \ \ \ $\%$ Reduce input to the largest activation in each unit
    \State \ \ \ \ $Output \gets \textbf{Max}(Input)$
    \\
    \State \ \ \ \ $\%$ Reshape to size equal the number of units
    \State \ \ \ \ $Output \gets \textbf{Reshape}(Input,(Nr \ Units))$
    \State \ \ \ \ $\textbf{return}\ Input$
    \end{algorithmic}
\end{algorithm}