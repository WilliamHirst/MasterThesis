\section{The search through Machine Learning}
\subsection{Model selection}
In this analysis I have chosen to compare 4 different \ac{ML}-models, dens-\acf{NN}, \acf{PNN},
\ac{LWTA} and XGBoost. The first three methods are all types of \ac{NN}. I have deliberately 
chosen to focus on \ac{NN} given that there is far more freedom in the design of the architecture
of a \ac{NN}, than compared to a XGboost. Additionally, this was motivated by the selfish reason
being that I found the networks more interesting to study and dissect. Therefore, most of the 
analysis, comparisons and discussion is focused on the networks, while XGBoost is included 
as a loose benchmark. 
\\
The choice of the three network architectures is motivated in wanting to compare a simple 
deep network, a network ensemble and a \ac{PNN}. I would assume that the optimal architecture
would be a network which combines elements from each, but for the purpose of discussion
and research I have chosen to keep them somewhat separate. 
\subsection{Creating custom layers}\label{subsec:CustomLayer}
The field of \ac{ML} is one of the most dynamic and fastest growing fields of research
today. This means, that regardless of the brave attempt made by voluntary contributors and
the people at Google\footnote{The developers of TensorFlow}, there will always be 
new and exciting \ac{ML} tools, not yet implemented in their library. This was also 
the case in this thesis. Specifically, in the case of non-dense layers I was forced
to dive into the world of \ac{ML} development and create my own implementation. 
\subsubsection*{Max-out}
TensorFlow have already implemented a very similar layer called MaxPooling1D. This does 
exactly what max-out (see subsection \ref{subsubsec:maxout}) does, but with minor differences. 
Additionally, I wanted the freedom to experiment with the implementation of the layer. The 
implementation of both max-out and channel-out (see subsection \ref{subsubsec:channelout}) 
was done by creating a custom activation function which is called inside a dense-layer. 
\\
In the code listing, \ref{lst:max_out} I have included the code implementation of the
activation function used to create the max-out layer. The listing shows how the function
takes the input from the previous layer, defines the new shape of what will become the 
output, groups the nodes in the size defined by number of units, then returns an output 
which includes only the largest activated node from each unit. By using this function 
in a dense-layer, said layer will act as a max-out layer. 
\lstset{style=Python}
\begin{lstlisting}[caption={Python implementation for the custom activation function used to define the max-out layer.},captionpos=b, label={lst:max_out}]
def call(self, inputs: tf.Tensor) -> tf.Tensor:
    # Passing input through weight kernel and adding bias terms
    inputs = gen_math_ops.MatMul(a=inputs, b=self.kernel)
    inputs = nn_ops.bias_add(inputs, self.bias)

    num_inputs = inputs.shape[0]
    if num_inputs is None:
        num_inputs = -1
    num_competitors = self.units // self.num_groups
    new_shape = [num_inputs, self.num_groups, num_competitors]

    # Reshaping outputs such that they are grouped correctly
    inputs = tf.reshape(inputs, new_shape)
    # Finding maximum activation in each group
    outputs = tf.math.reduce_max(inputs, axis=-1,keepdims=True)

    counter = tf.where(tf.equal(inputs, outputs), outputs, 0.)
    # Reshaping outputs to original input shape
    self.counter = tf.reshape(counter, [num_inputs, self.units])

    return tf.reshape(outputs,[num_inputs, self.num_groups])   
\end{lstlisting}
\subsubsection*{Channel-out}
For the activation function defined to create the channel-out layer, I again grouped
the nodes similarly as I did for max-out. Instead of returning the largest value from each 
unit, I used TensorFlow's function, $tf.greater\_equal$ to create a tensor with booleans. The 
booleans are chosen by comparing each unit to the largest value in that unit. By multiplying 
this tensor to the original input, I am left with the desired result of a tensor containing 
the largest activated nodes along with the rest whom are now set to zero. 
\lstset{style=Python}
\begin{lstlisting}[caption={Python implementation for the custom activation function used to define the channel-out layer.},captionpos=b, label={lst:channel_out}]
def channel_out(inputs, num_units = 200, axis=None, training=None):
    # Calculate the new shape of the layer after max_out.
    shape = inputs.get_shape().as_list()
    if shape[0] is None:
        shape[0] = -1
    if axis is None:  # Assume that channel is the last dimension
        axis = -1
    num_channels = shape[axis]
    
    if num_channels % num_units:
        raise ValueError('Number of features is not a multiple of num_units')
    shape[axis] = num_units
    shape += [num_channels // num_units]
    grouped = tf.reshape(inputs, shape)
    # Calculate the largest value in each unit and discard 
    top_vals = tf.reduce_max(grouped, -1, keepdims=True)
    isMax = tf.reshape(tf.greater_equal(grouped, top_vals), [shape[0], num_channels])
    output = tf.multiply(tf.cast(isMax,tf.float32), inputs)
    return output  
\end{lstlisting}
\subsection{Model Architecture}
When choosing a network architecture, there are several ways to proceed. One way is to apply a grid search.
A grid search is simply defining a grid of parameters to test, then running through all combinations and 
choosing the highest performer. With a sufficient amount of tests, a grid search should converge towards 
an optimal architecture. Grid search is very common and there exists a large range of very complex varieties \cite{GS}.
For my analysis I chose not to perform a grid search, for several reasons. The first being interpretability.
Understanding a \ac{NN} is already hard, allowing for complex and unique architectures would only add another layer
of mysticism. The second is the size of the data set. The larger the data set, the larger the amount of data 
would be needed to adequately perform tests for each combination of parameters. Not only is this time-consuming,
but trying to mediate this issue could lead to poor performance. The third and most important reason is that 
I wanted to experiment with the architectures. By manually tuning the parameters, I was able to achieve a far 
better understanding of the final architecture. 
\begin{figure}
    \makebox[0.9\linewidth][c]{%
    \centering
    \begin{subfigure}{1.1\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Illustrations/architecture.png}
    \end{subfigure}
    }
    \caption{A visual summary of the workflow and framework use for the 
    computational analysis. }
    \label{fig:arch}
\end{figure}
\subsection*{Dense Neural Network and PNN}
The purpose of the simple dense \ac{NN}, is to compare the more complex networks to what is usually considered a traditional \ac{NN}.
All layers are dense layers, meaning that all nodes in the previous layer are connected to the current layer, and likewise
the current layer is connected to the next. The general structure of the network is summarized in figure \ref{fig:arch}, with 
the network label \ac{NN}\footnote{Note that the dense network will henceforth be referred to as \ac{NN}}. The figure shows a \ac{DNN} with 
three hidden layers, all with 600 nodes each. All hidden layers utiliz the $LeakyReLU$ activation (see section \ref{subsec:activation})
with an $\alpha$ = 0.01. The architecture is designed to perform deep-training, and will train on a training set where all mass combinations 
are included\footnote{Contrary to training one network for each mass combination.}. 
\\
The \ac{PNN} architecture is like the name suggests, included to represent the model proposed by the article by Baldi et al. \cite{PNN}.
The architecture is illustrated in figure \ref{fig:arch}, with the label PNN. The figure shows a practically identical 
structure to the dense-\ac{NN}, with the only difference being in the input-layer. As was discussed in section \ref{subsec:PNN},
the \ac{PNN} includes the new physics signal free parameters\footnote{In our case, the masses of the \ac{BSM}-particles.} alongside the features
in the input layer.
\subsection*{MaxOut}
The MaxOut-network differs from the dense-network and the \ac{PNN} in that it uses an ensemble of networks. In figure \ref{fig:arch}
I have illustrated the MaxOut architecture, with the label MaxOut. The figure shows a network with 6 hidden layers, 3 MaxOut and 3 dropout.
The network alternates between drop out and MaxOut, starting with dropout and finishing with MaxOut. The MaxOut layers have 600 nodes each 
which reduce down to the 200 nodes with the largest activation in their respective groups. Each dropout layer has a dropout rate of 0.15.
\\
\subsection{Training and Validating data}
When building a \ac{ML}-model, the usual approach is to divide your data into three sets; training, validation and 
testing. The training set is used to tune the internal parameters of the model, i.e. the weights and biases of a \ac{NN} or the cuts of a \ac{DT}.
The validation set is used to tune the hyperparameters of the model, i.e. the architecture of the \ac{NN} or the maximum depth of the \ac{DT} etc.
The test set should only be used when the model is finished, and is used to benchmark the models' performance. In our case, the performance we are 
interested in, is the performance on the full \ac{MC}-bakgroundset and its comparison to the measured collision data.\footnote{See section INSERT 
THE APPROPRIATE SECTION.}. Therefore, in this analysis only two sets of data will be used, training and validation. In theory, one could even just 
use one data set (training) including all the data, but the second was added as a precaution to reduce overfitting when applied to the measured collision
data.
\\
The overarching strategy is summarized in the following points:
\begin{itemize}
    \item Shuffle the data set. 
    \item Split the data set in two, training ($80\%$) and validation ($20\%$)
    \item Scale the two data set such that the sum of the weights of the background is equal to the sum of the weights of the signal in each data set.
    \item Scale both data sets using the Standard Scalar approach (see section \ref{subsubsec:StandardScalar}) using the parameters of the training set 
          on both sets.
\end{itemize}
The first stem ensures an equal 

