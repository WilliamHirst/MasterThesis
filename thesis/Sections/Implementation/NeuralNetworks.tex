\section{The search through neural networks and sparse pathways}
\subsection{Creating custom layers}
The field of \ac{ML} is one of the most dynamic and fastest growing fields of research
today. This means, that regardless of the brave attempt made by voluntary contributors and
the people at Google \footnote{The developers of TensorFlow}, there will always be 
new and exciting \ac{ML} tools, not yet implemented in their library. This was also 
the case in this thesis. Specifically, in the case of non-dense layers I was forced
to dive into the world of \ac{ML} development and create my own implementation. 
\subsubsection*{Max-out}
TensorFlow have already implemented a very similar layer called MaxPooling1D. This does 
exactly what max-out does, but with minor differences. Additionally, to wanting to avoid 
these differences, I wanted the freedom to experiment with the implementation of the 
layer. The implementation of both max-out and channel-out was done by creating a custom 
activation function which is called inside a dense-layer. In the case of max-out, it is 
inside the activation function that the down scaling of nodes happens. 
\\
In the code listing, \ref{lst:max_out} I have included the code implementation of the
activation function used to create the max-out layer. The listing shows how the function
takes the input from the previous layer, defines the new shape of what will become the 
output, groups the nodes in the size defined by number of units, then return an output 
which includes only the largest activated node from each unit. By using this function 
in a dense-layer, said layer will act as a max-out layer. 
\lstset{style=Python}
\begin{lstlisting}[caption={Python implementation for the custom activation function used to define the max-out layer.},captionpos=b, label={lst:max_out}]
def call(self, inputs: tf.Tensor) -> tf.Tensor:
    # Passing input through weight kernel and adding bias terms
    inputs = gen_math_ops.MatMul(a=inputs, b=self.kernel)
    inputs = nn_ops.bias_add(inputs, self.bias)

    num_inputs = inputs.shape[0]
    if num_inputs is None:
        num_inputs = -1
    num_competitors = self.units // self.num_groups
    new_shape = [num_inputs, self.num_groups, num_competitors]

    # Reshaping outputs such that they are grouped correctly
    inputs = tf.reshape(inputs, new_shape)
    # Finding maximum activation in each group
    outputs = tf.math.reduce_max(inputs, axis=-1,keepdims=True)

    counter = tf.where(tf.equal(inputs, outputs), outputs, 0.)
    # Reshaping outputs to original input shape
    self.counter = tf.reshape(counter, [num_inputs, self.units])

    return tf.reshape(outputs,[num_inputs, self.num_groups])   
\end{lstlisting}
\subsubsection*{Channel-out}
For the activation function defined to create the channel-out layer, I again grouped
the nodes similarly as I did for max-out. Instead of returning the largest value from each 
unit, I used TensorFlow's function, $tf.greater\_equal$ to create a tensor with booleans. The 
booleans are chosen by comparing each unit to the largest value in that unit. By multiplying 
this tensor to the original input, I am left with the desired result of a tensor containing 
the largest activated nodes along with the rest whom are now set to zero. 
\lstset{style=Python}
\begin{lstlisting}[caption={Python implementation for the custom activation function used to define the channel-out layer.},captionpos=b, label={lst:channel_out}]
def channel_out(inputs, num_units = 200, axis=None, training=None):
    # Calculate the new shape of the layer after max_out.
    shape = inputs.get_shape().as_list()
    if shape[0] is None:
        shape[0] = -1
    if axis is None:  # Assume that channel is the last dimension
        axis = -1
    num_channels = shape[axis]
    
    if num_channels % num_units:
        raise ValueError('Number of features is not a multiple of num_units')
    shape[axis] = num_units
    shape += [num_channels // num_units]
    grouped = tf.reshape(inputs, shape)
    # Calculate the largest value in each unit and discard 
    top_vals = tf.reduce_max(grouped, -1, keepdims=True)
    isMax = tf.reshape(tf.greater_equal(grouped, top_vals), [shape[0], num_channels])
    output = tf.multiply(tf.cast(isMax,tf.float32), inputs)
    return output  
\end{lstlisting}
