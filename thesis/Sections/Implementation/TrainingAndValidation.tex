\newpage
\section{Model Training and Validation}
\subsection{Training and Validating Data}\label{subsec:TraVal}
When building an \ac{ML}-model the usual approach is to divide your data into three sets; training, validation and 
testing. The training set is used to tune the internal parameters of the model, i.e. the weights and biases of a \ac{NN} or the cuts of a \ac{DT}.
The validation set is used to tune the hyperparameters of the model, for example the architecture of the \ac{NN} or the maximum depth of the \ac{DT}.
The test set should only be used when the model is finished, and is used to benchmark the model's performance. In our case, the performance we are 
interested in is the performance on the full \ac{MC}-background set and its comparison to the measured collision data. 
Therefore, in this analysis only two sets of data will be used, training and validation where both are sampled from the simulated
\ac{MC} data set. The validation set was added as a precaution to reduce overfitting when applied to the measured collision data.
\\
The overarching strategy in creating the training and validation set is summarized in the following steps:
\begin{enumerate}
    \item Shuffle the data set. 
    \item Split the data set in two, training ($80\%$) and validation ($20\%$)
    \item Scale the two data set such that the sum of the weights of the background is equal to the sum of the weights of the signal in each data set.
    \item Scale both data sets using the Standard Scalar approach (see section \ref{subsubsec:StandardScalar}) using the parameters (the mean and standard deviation) 
    of the training set on both sets.
\end{enumerate}
The first step ensures an equal distribution of processes in both data sets. The $80-20\%$ is a popular practice in \ac{ML} (see \cite{8020}) and was chosen 
here for convenience. The third step refers to the sample-weights introduced in section \ref{subsec:SGD}. The sample-weights are included in the simulated data
to scale each simulated event based on the (among other things) probability of the collision, otherwise known as the cross-section. The scaling of the 
weights is done to ensure an equal prioritization for background classification and signal classification. Without weighting, an imbalance between signal and background would 
would motivate the model to tune more towards one distribution in the feature space than another. The final step is motivated in the section regarding data handling \ref{subsubsec:StandardScalar}. 

\subsection{Training Strategy}\label{subsec:TrainingStrategy}
To best compare the different \ac{ML}-models, I decide to apply the same training strategy to all (including the \ac{BDT}). The strategy is simply to train the model 
with the training set, then apply an early-stopping algorithm (see section \ref{subsec:EarlyStopping}) with a cut-off based on the performance on the validation set. For 
every epoch the model makes a prediction on the validation data set, and logs the results. If more than 10 epochs go by without improving on the epoch with the best result,
training stops and the weights of the epoch corresponding to the best performance on the validation set are reset. This strategy was repeated for the \ac{BDT}, but logging 
for each new additional tree instead of epoch.
\\
The performance from each epoch was measured in \ac{AUC} (see section \ref{subsec:AUC}), where the \ac{AUC} was calculated with the same weighting as in training ($50\%$ signal
and $50\%$ background). The distribution of signal vs background is important when studying \ac{AUC}, as it will greatly affect the value. For example, a classifier which predicts 
both signal and background to be background is a poor classifier. But the larger the amount of background relative to signal, the higher the \ac{AUC} would be. 