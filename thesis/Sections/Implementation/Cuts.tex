\section{Data Preprocessing}\label{subsec:Cuts}
To allow for deep learning and a thorough analysis one must try and keep
as much of the data as possible. At the same time, including large amounts
of irrelevant data can be both redundant and destructive\footnote{By including 
large amounts of irrelevant data, you risk the \ac{ML}-model tuning unnecessarily 
to remove easily reducible background, which could compromise performance on irreducible 
background.}. Therefore, preprocessing in the form of preselection cuts are necessary. 
The cuts applied in the analysis were grouped in two definitions, baseline and signal. 
The baseline requirements are written in table \ref{table:BL} and the signal requirements are written 
in table \ref{table:SG}. Both sets of requirements were taken from the ATLAS article from 2022 \cite{franchini_search_2019}.
Given the definitions we demand that each event contains exactly three signal (\ref{table:SG}) and 
three baseline leptons (\ref{table:BL}), thereby removing any event with more or less. 
\\
Leptons are identified in the detector by using a likelihood-based method combining
information from different parts of the detector. The criteria of Loose or Tight 
identification are simply different thresholds in the discriminant, where Loose is 
defined as a lower threshold than Tight \cite{Aaboud_2019}. The overlap removal is used to solve any cases
where the same lepton has been reconstructed as both a muon and an electron. The boolean
of $lepPassOr$ simply applies a set of requirements to avoid any double counting. The cut for
the longitudinal track parameters, $z_0$ is applied to ensure that the leptons originate from the 
primary vertex.
\\
As for the requirements for the signal leptons, we require all baseline requirements are passed 
with the addition of a few more. We require Loose isolation for both electrons and muons. This means
requiring isolation criteria for a cone around the lepton is used to suppress \ac{QCD}-background events
and reduce fake leptons. Similarly to the $z_0$-cut, the transverse track parameter is also used to ensure 
origin from primary vertex.
\begin{figure}[H]
    \renewcommand\figurename{Table}
    \centering
    \makebox[\linewidth][c]{
        \begin{subfigure}{.475\textwidth}
            $
            \begin{array}{ccc}
                \hline \text { Requirement } & \text { Baseline electrons } & \text { Baseline muons } \\
                \hline\text{Identification} & - & \text{Loose} \\
                \text { Overlap Removal } & \text{lepPassOR} & \text{lepPassOR} \\
                \eta-\text { cut } & |\eta|<2.47 & |\eta|<2.7  \\
                \left|z_0 \sin (\theta)\right| \text { cut } & \left|z_0 \sin (\theta)\right|<0.5 \mathrm{~mm} & \left|z_0 \sin (\theta)\right|<0.5 \mathrm{~mm} \\
                \hline
            \end{array}
            $
            \caption{}
            \label{table:BL}
        \end{subfigure}
        \hspace{2.5cm}
        \begin{subfigure}{.475\textwidth}
            $
            \begin{array}{ccc}
                \hline \text { Requirement } & \text { Signal electrons } & \text { Signal muons } \\
                \hline\text{Baseline} & \text{yes} & \text{yes} \\
                \text{Identification} & \text{Tight} & - \\
                \text{Isolation} & \text{LooseVarRad} & \text{LooseVarRad}  \\
                \left|d_0\right| / \sigma_{d_0} \text { cut } & \left|d_0\right| / \sigma_{d_0}<5.0 & \left|d_0\right| / \sigma_{d_0}<3.0 \\
                \hline
            \end{array}
            $
            \caption{}
            \label{table:SG}
        \end{subfigure}
        
    }
    \caption{Two tables displaying the baseline \ref{table:BL} and signal \ref{table:SG} requirements applied 
    to the data as part of the preprocessing.}
\end{figure}
In addition to the simple cuts, we must insure a good comparison between
\ac{MC}- and real data. Often one finds large deviation between the two in the case
of either very large or very small transverse momentum, $P_t$. The latter case can often be caused by 
poor reconstruction or miss identification. These are issues we aim to solve by checking
different triggers. 
\\
In the earlier parts of the analysis, I discovered that the data set I was given, did not contain the correct information 
regarding the triggers. After spending some time trying to compensate for this, my Supervisor (Eirik Gramstad) suggested an alternative.
Instead of filtering using the triggers, an additional criterion of at least two leptons containing $P_t>20Gev$ was placed on the data,
as it was suggested that this would be relatively equivalent.
% Given our data set is composed of different data sets spread over
% several years, different triggers are used. 
% \begin{table}
%     \centering
%     $
%     \begin{array}{ccc}
%         \hline \text { 2015 } & \text { 2016 } & \text { 2017 + 2018 } \\
%         \hline
%         \text{HLT\_2e12\_lhloose\_L12EM10VH} & \text{HLT\_2e17\_lhvloose\_nod0} & \text{HLT\_2e17\_lhvloose\_nod0\_L12EM15VHI} \\
%         \text{HLT\_e17\_lhloose\_mu14} & \text{HLT\_e17\_lhloose\_nod0\_mu14} & \text{HLT\_e17\_lhloose\_nod0\_mu14} \\
%         \text{HLT\_mu18\_mu8noL1} & \text{HLT\_mu22\_mu8noL1} & \text{HLT\_mu22\_mu8noL1} \\
%         & & \text{ HLT\_2e24\_lhvloose\_nod0}\\

%         \hline
%     \end{array}
%     $
%     \caption{Trigger requirments for events produced in their respective years.}
% \label{table:Triggers}
% \end{table}
