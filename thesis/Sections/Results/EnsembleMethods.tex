\section{Ensemble methods}\label{sec:Ensemble}
\subsection{Visualizing Sparse Pathways}\label{subsec:Viz}
As mentioned in section \ref{subsec:CustomLayer}, the channel-out, \ac{SCO} and maxout layers applied in this 
analysis were created by me using the TensorFlow \ac{API}'s. As such, I found it imperative to make
sure that the layers worked as intended. To do this I created a small network with three layers, with 8 
nodes each, all applying MaxOut layers with 4, 2 and 4 units respectively. In this section I will 
dissect the activations of said network before and after training.
\\
In figure \ref{fig:BTraining}, I have plotted the activation of 100 randomly sampled events, 50 
background and 50 signal for an untrained model. Adjacent, I plotted the resulting distribution of the 
output. From the figure we observe little to no deviation between the activation from the signal and 
the background. This is mirrored in the distribution of the output which is centered around the middle 
of the range. This result is as expected, given that an untrained model holds no knowledge of the data and 
simply applies a random set of weights. A small exception from this result can be found for larger output values, 
where we observe a small distribution of signal values. This is due to the signal having some inherit differences
to the background (for example high $E_t^{miss}$).
\\
In figure \ref{fig:ATraining}, I plotted a similar plot as described above, but using a trained model.
In this figure the output is far more separated, and we see noticeable differences in the activation 
of nodes. To highlight the difference in activation, I drew two new figures where the signal 
(\ref{fig:ATrainingSig}) and background (\ref{fig:ATrainingBkg}) were drawn individually.
In the two figures we notice that there is still a noticeable variation in the activation for
both signal and background. This is due to outliers in the data, but also the variation of trend.
Regardless, there are still clear trends in activation which lets us know that the model has found 
specific paths in the network for signal and background separately. Most noticeably, the two unit 
in the middle hidden layer highlight this fact. In the case of the signal, the upper unit show a clear 
favoritism to the second bottom node. For the background this is also partly true, but with far more 
spread in the other nodes. Similarly, in the bottom unit (in the same layer), the background shows large
activation in the uppermost node, while the signal data does not. To conclude, the maxout layer does in deed 
find trend specific paths through the network which will aid in separating the output for the signal and the background.\\
\begin{figure}
    \makebox[\linewidth][c]{%
    \centering
    \begin{subfigure}{.6\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/NetworkVis/BeforeTraining.pdf}
        \caption{}
        \label{fig:BTraining}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.6\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/NetworkVis/AfterTraining.pdf}
        \caption{}
        \label{fig:ATraining}
    \end{subfigure}
    }
    \caption[A calculated visualization of the activation of a three layer MaxOut network, before and after training.]{
    A calculated visualization of a three layer MaxOut network. Each path 
    represents a data point where all connected nodes were the largest activation in their respective 
    unit. The distribution on the far right represent the output distribution. The figure to the left
    (\ref{fig:ATrainingSig}) is the result before training and the figure to right (\ref{fig:ATrainingBkg})
    is after.}
\end{figure}


\begin{figure}
    \makebox[\linewidth][c]{%
    \centering
    \begin{subfigure}{.6\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/NetworkVis/AfterTrainingSig.pdf}
        \caption{}
        \label{fig:ATrainingSig}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.6\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/NetworkVis/AfterTrainingBkg.pdf}
        \caption{}
        \label{fig:ATrainingBkg}
    \end{subfigure}
    }
    \caption[A calculated visualization of the activation of a three layer MaxOut network, after training and displaying the
    signal and background separately.]{A calculated visualization of a three layer MaxOut network. Each path 
    represents a data point where all connected nodes were the largest activation in their respective 
    unit. The distribution on the far right represent the output distribution and the figure with blue 
    paths (left) \ref{fig:ATrainingSig} is the result of signal, and the pink paths (right) is a result from background 
    \ref{fig:ATrainingBkg}.} 
    \label{fig:NetDist1}
\end{figure}

\begin{figure}
    \makebox[\linewidth][c]{%
    \centering
    \begin{subfigure}{.6\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/NetworkVis/AfterTrainingSig50250.pdf}
        \caption{}
        \label{fig:ATrainingSig50250}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.6\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/NetworkVis/AfterTrainingSig200300.pdf}
        \caption{}
        \label{fig:ATrainingSig200300}
    \end{subfigure}
    }
    \caption[A calculated visualization of the activation of a three layer MaxOut network, after training and displaying the
    the results for two signal with each their own mass combination.]{A calculated visualization of a trained MaxOut network 
    with three hidden layers. Each path represents a data point where all connected nodes were the largest activation in their respective 
    unit. The figure to the left (\ref{fig:ATrainingSig50250}) is a result of signal with 
    ($\tilde{\chi}_1=50$, $\tilde{\chi}_2=250$GeV) and the right (\ref{fig:ATrainingSig200300}) 
    ($\tilde{\chi}_1=200$, $\tilde{\chi}_2=300$GeV).}
    \label{fig:NetVisSigComp}
\end{figure}
Ideally, we would want the model to not only be able to separate the signal from the background, but do so in 
a way which allows the network to store the different trends within the signal. In section \ref{subsubsec:Channel-Out},
I described this ability as long term memory. One way to implement long term memory is through a high number of parameters,
but through the \ac{LWTA} layers, we can additionally do this through specific paths. To study this effect, I have created similar 
figures as discussed in the paragraphs above, but this time only including one mass combination in the figure. Figures
\ref{fig:ATrainingSig50250} and \ref{fig:ATrainingSig200300} present the results for the mass combinations 
($\tilde{\chi}_1=50$, $\tilde{\chi}_2=250$GeV) and ($\tilde{\chi}_1=200$, $\tilde{\chi}_2=300$GeV) respectively.
By comparing the figures, we see a small but noticeable difference in paths. Again the middle hidden layer seems to be 
the differentiating factor. To highlight the differences, I have included a cutout, comparing the two highest nodes in the bottom unit 
for the middle layer of both figures, \ref{fig:ATrainingSig50250Zoom} and \ref{fig:ATrainingSig200300Zoom} (in the same order).
By studying the cutouts, we can conclude that the maxout layer not only applies a form of regularization, but also increases the long term 
memory of the model through trend specific paths through the network.
\begin{figure}
    \makebox[\linewidth][c]{%
    \centering
    \hfill
    \hfill
    \begin{subfigure}{.1\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/NetworkVis/AfterTrainingSig50250Zoom.pdf}
        \caption{}
        \label{fig:ATrainingSig50250Zoom}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.104\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/NetworkVis/AfterTrainingSig200300Zoom.pdf}
        \caption{}
        \label{fig:ATrainingSig200300Zoom}
    \end{subfigure}
    \hfill
    \hfill
    }
    \caption[A calculated visualization of the activation of a three layer MaxOut network, after training and displaying the
    the results for two signal with each their own mass combination, highlighting the difference between two specific nodes.]{
    A cut-out of the fifth and sixth node (counting from the top) in the second hidden layer, activated 
    by the signal with ($\tilde{\chi}_1=50$, $\tilde{\chi}_2=250$GeV) \ref{fig:ATrainingSig50250Zoom} and 
    ($\tilde{\chi}_1=200$, $\tilde{\chi}_2=300$GeV) \ref{fig:ATrainingSig200300Zoom}.}
    \label{fig:NetVisZoom}
\end{figure}
\subsection{Training History and Overfitting}\label{subsec:Overfitting}
In section \ref{sec:Regularization} I described how creating ensembles of networks is a form of regularization. Therefore,
it is of interest to study the relationship between performance on the training set and the validation set for our ensemble 
methods. In figure \ref{fig:History} I drew plots displaying the performance on the training and validation scores after each 
epoch (50 in total), as measured in \ac{AUC} for both a dense \ac{NN} (\ref{fig:NNHist}) and maxout (\ref{fig:MaxOutHist}).
\\ 
In figure \ref{fig:NNHist} we can observe that a deep, dense \ac{NN} reaches a maximum in performance for the validation set after only 
a couple epochs. This peak is then followed by a quick drop in performance, while the training set increases in performance. 
The drop in performance for the validation set and increase in performance for the training set is a sure sign of overfitting. 
\\
In figure \ref{fig:MaxOutHist} we observe that an ensemble method (in this case the maxout) displays a different trend. For the first 
10 epochs the model increases in performance for both data sets. After this, the validation set does not decrease in performance, but 
simply holds stable while the training set increases. This is a sign that the model is not experiencing overfitting. Furthermore, the 
training history of the maxout model raises an interesting point. From experience, I know that the maxout model reaches a peak in performance
on the validation set after approximately 15-20 epochs. Given that the performance after said peak is stable, it is plausible that continuing 
training will not worsen the performance on the full said, or even lead to overfitting, but improve. This is definitely an interesting possibility
for the \ac{LWTA} layers, but will not be studied in this thesis due to time constraints.
\\
In section \ref{subsec:TrainingStrategy}, I discussed the training strategy used when training all models in this analysis. I mentioned 
the implementation of an early stopping criterion which makes sure that the model only continues to train as long as the performance on the 
validation data set increases. By studying the subfigures in figure \ref{fig:History}, we can deduce that the ensemble methods will not only 
be able to avoid overfitting, but will as a consequence be allowed to train much longer than the other models.
\begin{figure}
    \makebox[\linewidth][c]{%
    \centering
    \begin{subfigure}{.45\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/SUSY/History/NNHistory.pdf}
        \caption{}
        \label{fig:NNHist}
    \end{subfigure}
    \begin{subfigure}{.45\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/SUSY/History/MaxOutHistory.pdf}
        \caption{}
        \label{fig:MaxOutHist}
    \end{subfigure}
    }
    \caption[A plot comparing the \acs{AUC} score made after each epoch on both the training and validation set, between a dense \ac{NN} 
    and maxout model.]{A plot displaying the \ac{AUC} score made after each epoch on both the training and validation set. 
    Figure \ref{fig:NNHist} shows the results from the dense \ac{NN} and figure \ref{fig:MaxOutHist} shows
    the results from a maxout network.}
    \label{fig:History}
\end{figure}
\subsection{Comparing Achieved Sensitivity between Ensemble Methods}
In this section I will present and discuss the performance of the three different networks discussed in section 
\ref{subsec:Ensembles}, channel-out, \ac{SCO} and maxout. The results presented in this section were made using the
original signal set (see section \ref{subsec:signal}), the training strategy described in section \ref{subsec:TrainingStrategy}
and the architectures described in section \ref{subsec:arch}.
\\
In figure \ref{fig:MaxOutGridSig}, I present the achieved sensitivity of the maxout model using the original signal set. 
The grid shows the same trends as the previous models, i.e. the preference in the higher statistics mass combinations. By comparing 
the results from the maxout model to the deep, dense network presented in figure \ref{fig:NNGridSig}, we discern that the dense 
network outperforms the maxout model (ever so slightly) for the higher statistic mass combinations. It is plausible that this is due 
to the difference in depth. The dense network utilizes 3 hidden layers of 600 nodes each, while the maxout, although built with the 
same architecture, only utilizes 200 nodes per layer when propagating input through the network. This could result in the dense network 
being able to more deeply tune for the trends in the higher statistics combinations.
\\
However, the most interesting result for the maxout model, is its ability to tune for all mass combinations. Although the dense network
outperformed the maxout model for the high statistics combinations, the maxout model outperformed the dense for most other combination (26/30).
This result can be credited to two factors. The first being maxout's effect as a form of regularization. In the previous 
section (see section \ref{subsec:Overfitting}), I presented how through maxout's regularization abilities, it would be able to 
uphold the early stopping criteria for a larger number of epochs. The second factor is maxout's innate long term memory which was studied
in section \ref{subsec:Viz}. From figure \ref{fig:MaxOutGridSig}, we are able to deduce that although the maxout model is not able to tune 
to the same depth for the lower masses, it is able to achieve a large level of generalizability through reducing overfitting and increasing long term memory.\\
\begin{figure}
    \makebox[\linewidth][c]{%
    \centering
    \begin{subfigure}{.65\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/SUSY/Grid/MaxOutGridSig.pdf}
    \end{subfigure}
    }
    \caption{A grid displaying the achieved significance on the original signal set, using the signal region 
    created by the maxout network.}
    \label{fig:MaxOutGridSig}
\end{figure}
In appendix \ref{appendix:Ensembles} I have included grids displaying the achieved sensitivity for both channel-out and \ac{SCO}. Both 
models demonstrate similar performance to the maxout layer. To compare the three methods, I created a "pie-plot". A pie-plot 
compares the achieved sensitivity between several models and displays the results for each individual mass combination.  In figure 
\ref{fig:EnsembleComp} I present the pie-plot comparing maxout, channel-out and \ac{SCO}. Each mass combination includes a pie, where 
the size of each "slice" represents the relative size of the significance compared to the other methods. For example, if a slice occupies 
half of the pie, then the method corresponding to that slice achieved a significance equal to the sum of the significance of the other methods.
The color surrounding each pie marks which method achieved the highest sensitivity for the respective combination.
\\
By studying the pie-plot in figure \ref{fig:EnsembleComp}, we can deduce that maxout model outperforms the other two in most of the mass 
combination (24/30). From the sizes of each slice, we can deduce that all three models seem relatively equal in performance, maxout only 
outperforming the others by a small fraction. The most interesting observation from the pie-plot is the performance from the \ac{SCO}, as 
this model was in-part created by me. Except the mass combinations where maxout was the most sensitive, \ac{SCO} was the highest performing model. 
Most interestingly, it outperformed the channel-out model, which is the model most similar to \ac{SCO}, in 9/30 (see figure \ref{fig:SCOCO}). 
In hindsight, I believe that by removing the \ac{SCO} during prediction (similar to what is done for dropout), the \ac{SCO} layer would greatly 
approve in performance, on a count of the fact that the performance on each event is dependent on the random choice of unit for that prediction.
Nonetheless, this analysis is an indicator that although the maxout model was the highest performing model, the \ac{SCO} layer shows 
great promise and should be further explored in further analysis. 
\begin{figure}
    \makebox[\linewidth][c]{%
    \centering
    \begin{subfigure}{.75\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/SUSY/Comparison/EnsemblesNetworkComp.pdf}
    \end{subfigure}
    }
    \caption[A sensitivity comparison between the ensemble networks (maxout, \ac{SCO}, channel-Out) on the original 
    signal data.]{A sensitivity comparison between the ensemble networks (maxout, \ac{SCO}, channel-Out) on the original 
    signal data. The size of the "pie" represents the relative size of the significance and the color around each 
    point displays the method with the largest sensitivity for the respective combination.}
    \label{fig:EnsembleComp}
\end{figure}

