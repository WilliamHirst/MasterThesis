\section{Ensemble methods}\label{sec:Ensemble}
\subsection{Visualizing Sparse Pathways}\label{subsec:Viz}
As mentioned in section \ref{subsec:CustomLayer}, the channel-out, \ac{SCO} and maxout layers applied in this 
analysis were created by me using the TensorFlow \ac{API}'s. As such, I found it imperative to make
sure that the layers worked as intended. To do this I created a small network with three layers, with 8 
nodes each, all applying MaxOut layers with 4, 2 and 4 groups respectively. In this section I will 
dissect the activations of said network before and after training.
\\
In figure \ref{fig:BTraining}, I have plotted the activation of 100 randomly sampled collisions, 50 
background and 50 signal for an untrained model. Adjacent, I plotted the resulting distribution of the 
output. From the figure we observe little to no deviation between the activation from the signal and 
the background. This is mirrored in the distribution of the output which is centered around the middle 
of the range. A small exception can be found for larger output values, where we observe a small distribution
of signal values. This is due outliers in the signal data set.
\\
In figure \ref{fig:ATraining}, I plotted a similar plot as described above, but using a trained model.
In this figure the output is far more separated, and we see noticeable differences in the activation 
of nodes. To highlight the difference in activation, I drew two new figures where the signal 
(\ref{fig:ATrainingSig}) and background (\ref{fig:ATrainingBkg}) were drawn individually.
In the two figures we notice that there is still a noticeable variation in the activation for
both signal and background. This is due to outliers in the data, but also the variation of trend.
Regardless, there are still clear trends in activation which lets us know that the model has found 
specific paths in the network for signal and background separately. Most noticeably, the two group 
in the middle hidden layer highlight this fact. In the case of the signal, the upper group show a clear 
favoritism to the second bottom node. For the background this is also partly true, but with far more 
spread in the other nods. Similarly, in the bottom group (in the same layer), the background shows large
activation in the uppermost node, while the signal data does not. 
\begin{figure}
    \makebox[\linewidth][c]{%
    \centering
    \begin{subfigure}{.6\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/NetworkVis/BeforeTraining.pdf}
        \caption{}
        \label{fig:BTraining}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.6\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/NetworkVis/AfterTraining.pdf}
        \caption{}
        \label{fig:ATraining}
    \end{subfigure}
    }
    \caption{A calculated visualization of a 3 layer MaxOut network. Each path 
    represents a data point where all connected nodes were the largest activation in their respective 
    group. The distribution on the far right represent the output distribution. The figure to the left
    (\ref{fig:ATrainingSig}) is the result before training and the figure to right (\ref{fig:ATrainingBkg})
    is after.}. 
\end{figure}


\begin{figure}
    \makebox[\linewidth][c]{%
    \centering
    \begin{subfigure}{.6\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/NetworkVis/AfterTrainingSig.pdf}
        \caption{}
        \label{fig:ATrainingSig}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.6\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/NetworkVis/AfterTrainingBkg.pdf}
        \caption{}
        \label{fig:ATrainingBkg}
    \end{subfigure}
    }
    \caption{A calculated visualization of a 3 layer MaxOut network. Each path 
    represents a data point where all connected nodes were the largest activation in their respective 
    group. The distribution on the far right represent the output distribution and the figure with blue 
    paths \ref{fig:ATrainingSig} is the result of signal and the figure pink \ref{fig:ATrainingBkg}.
    } 
    \label{fig:NetDist1}
\end{figure}

\begin{figure}
    \makebox[\linewidth][c]{%
    \centering
    \begin{subfigure}{.6\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/NetworkVis/AfterTrainingSig50250.pdf}
        \caption{}
        \label{fig:ATrainingSig50250}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.6\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/NetworkVis/AfterTrainingSig200300.pdf}
        \caption{}
        \label{fig:ATrainingSig200300}
    \end{subfigure}
    }
    \caption{A calculated visualization of a trained MaxOut network with 3 hidden layers. Each path 
    represents a data point where all connected nodes were the largest activation in their respective 
    group. The figure to the left (\ref{fig:ATrainingSig50250}) is a result of signal with 
    ($\tilde{\chi}_1=50$, $\tilde{\chi}_2=250$GeV) and the right (\ref{fig:ATrainingSig200300}) 
    ($\tilde{\chi}_1=200$, $\tilde{\chi}_2=300$GeV).}
    \label{fig:NetVisSigComp}
\end{figure}

\begin{figure}
    \makebox[\linewidth][c]{%
    \centering
    \hfill
    \hfill
    \begin{subfigure}{.1\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/NetworkVis/AfterTrainingSig50250Zoom.pdf}
        \caption{}
        \label{fig:ATrainingSig50250Zoom}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.104\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/NetworkVis/AfterTrainingSig200300Zoom.pdf}
        \caption{}
        \label{fig:ATrainingSig200300Zoom}
    \end{subfigure}
    \hfill
    \hfill
    }
    \caption{A cut-out of the fifth and sixth node (counting from the top) in the second hidden layer, activated 
    by the signal with ($\tilde{\chi}_1=50$, $\tilde{\chi}_2=250$GeV) \ref{fig:ATrainingSig50250Zoom} and 
    ($\tilde{\chi}_1=200$, $\tilde{\chi}_2=300$GeV) \ref{fig:ATrainingSig200300Zoom}.}
    \label{fig:NetVisZoom}
\end{figure}
\subsection{Training History and Overfitting}\label{subsec:Overfitting}
In section \ref{sec:Regularization} I described how creating ensembles of networks is a form of regularization. Therefore,
it is of interest to study the relationship between performance on the training set and the validation set for our ensemble 
methods. In figure \ref{fig:History} I drew plots displaying the performance on the training and validation scores after each 
epoch (50 in total), as measured in \ac{AUC} for both a dense \ac{NN} (\ref{fig:NNHist}) and maxout (\ref{fig:MaxOutHist}).
\\ 
In figure \ref{fig:NNHist} we can observe that a deep, dense \ac{NN} reaches a maximum in performance for the validation set after only 
a couple epochs. This peak is then followed by a quick drop in performance, while the training set increases in performance. 
The drop in performance for the validation set and increase in performance for the training set is a sure sign of overfitting. 
\\
In figure \ref{fig:MaxOutHist} we observe that an ensemble method (in this case the maxout) displays a different trend. For the first 
10 epochs the model increases in performance for both data sets. After this, the validation set does not decrease in performance, but 
simply holds stable while the training set increases. This is a sign that the model is not experiencing overfitting.
\\
In section \ref{subsec:TrainingStrategy}, I discussed the training strategy used when training all models in this analysis. I mentioned 
the implementation of an early-stopping criteria which makes sure that the model only continues to train as long as the performance on the 
validation data set increases. By studying the subfigures in figure \ref{fig:History}, we can deduce that the ensemble methods will not only 
be able to avoid overfitting, but will as a consequence be allowed to train much longer than the other models.
\begin{figure}
    \makebox[\linewidth][c]{%
    \centering
    \begin{subfigure}{.45\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/SUSY/History/NNHistory.pdf}
        \caption{}
        \label{fig:NNHist}
    \end{subfigure}
    \begin{subfigure}{.45\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/SUSY/History/MaxOutHistory.pdf}
        \caption{}
        \label{fig:MaxOutHist}
    \end{subfigure}
    }
    \caption{A plot displaying the \ac{AUC} score made after each epoch on both the training and validation set. 
    Figure \ref{fig:NNHist} shows the results from the dense \ac{NN} and figure \ref{fig:MaxOutHist} shows
    the results from a maxout network.}
    \label{fig:History}
\end{figure}
\subsection{Comparing achieved Sensitivity between Ensemble methods}
In this section I will present and discuss the performance of the three different networks discussed in section 
\ref{subsec:Ensembles}, channel-out, \ac{SCO} and maxout. The results presented in this section were made using the
original signal set (see section \ref{subsec:signal}), the training strategy described in section \ref{subsec:TrainingStrategy}
and the architectures described in section \ref{subsec:arch}.
\\
In figure \ref{fig:MaxOutGridSig}, I present the achieved sensitivity of the maxout model using the original signal set. 
The grid shows the same trends as the previous models, i.e. the preference in the higher statistics mass combinations. By comparing 
the results from the maxout model to the deep, dense network presented in figure \ref{fig:NNGridSig}, we discern that the dense 
network outperforms the maxout model (ever so slightly) for the higher statistic mass combinations. It is plausible that this is due 
to the difference in depth. The dense network utilizes 3 hidden layers of 600 nodes each, while the maxout, although built with the 
same architecture, only utilizes 200 nodes per layer when propagating input through the network. This could result in the dense network 
being able to more deeply tune for the trends in the higher statistics combinations.
\\
However, the most interesting result for the maxout model, is its ability to tune for all mass combinations. Although the dense network
outperformed the maxout model for the high statistics combinations, the maxout model outperformed the dense for most other combination (26/30).
This result can be credited to two factors. The first being maxout's effect as a form of regularization. In the previous 
section (see section \ref{subsec:Overfitting}), I presented how through maxout's regularization abilities, it would be able to 
uphold the early-stopping criteria for a larger number of epochs. The second factor is maxout's innate training memory\footnote{By training
memory, I refer to the models' ability to tune towards several trends without reducing in performance.} which was tested in section \ref{subsec:Viz}. 
From figure \ref{fig:MaxOutGridSig}, we are able to deduce that although the maxout model is not able to training to the same depth 
for the lower masses, it is able to achieve a large level of generalizability through reducing overfitting and increasing training memory.\\
\begin{figure}
    \makebox[\linewidth][c]{%
    \centering
    \begin{subfigure}{.65\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/SUSY/Grid/MaxOutGridSig.pdf}
    \end{subfigure}
    }
    \caption{A grid displaying the achieved significance on the original signal set, using the signal region 
    created by the \emph{MaxOut} network.}
    \label{fig:MaxOutGridSig}
\end{figure}
In appendix \ref{appendix:Ensembles} I have included grids displaying the achieved sensitivity for both channel-out and \ac{SCO}. Both 
models demonstrate similar performance to the maxout layer. To compare the three methods, I created a "pie-plot". A pie-plot 
compares the achieved sensitivity between several models and display the results for each individual mass combination.  In figure 
\ref{fig:EnsembleComp} I present the pie-plot comparing maxout, channel-out and \ac{SCO}. Each mass combination includes a pie, where 
the size of each "slice" represents the relative size of the significance compared to the other methods. The color surrounding each 
pie marks which method achieved the highest sensitivity for the respective combination.
\\
By studying the pie-plot in figure \ref{fig:EnsembleComp}, we can deduce that maxout model outperforms the other two in most of the mass 
combination (24/30). From the sizes of each "slice", we can observe that all three models seem relatively equal in performance, maxout only 
outperforming the others by a small fraction. The most interesting observation from the pie-plot is the performance from the \ac{SCO}, as 
this model was in-part created by me. Except the mass combinations where maxout was the most sensitive, \ac{SCO} outperformed 
the rest. This is an indicator that although the method seemed not to be the most optimal ensemble method for this analysis, it does 
indeed show great promise and should be further explored in further analysis. 
\begin{figure}
    \makebox[\linewidth][c]{%
    \centering
    \begin{subfigure}{.75\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/SUSY/Comparison/EnsemblesNetworkComp.pdf}
    \end{subfigure}
    }
    \caption{A sensitivity comparison between the ensemble methods (maxout, \ac{SCO}, channel-Out) on the original 
    signal data. The size of the "pie" represents the relative size of the significance and the color around each 
    point displays the method with the largest sensitivity for the respective combination.}
    \label{fig:EnsembleComp}
\end{figure}

