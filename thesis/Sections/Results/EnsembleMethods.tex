\section{Ensemble methods}
\subsection{Visualizing Sparse Pathways}
As mentioned in section \ref{subsec:CustomLayer}, the channel-out, \ac{SCO} and maxout layers applied in this 
analysis were created by me using the TensorFlow \ac{API}'s. As such, I found it imperative to make
sure that the layers worked as intended. To do this I created a small network with three layers, with 8 
nodes each, all applying MaxOut layers with 4, 2 and 4 groups respectively. In this section I will 
dissect the activations of said network before and after training.
\\
In figure \ref{fig:BTraining}, I have plotted the activation of 100 randomly sampled collisions, 50 
background and 50 signal for an untrained model. Adjacent, I plotted the resulting distribution of the 
output. From the figure we observe little to no deviation between the activation from the signal and 
the background. This is mirrored in the distribution of the output which is centered around the middle 
of the range. A small exception can be found for larger output values, where we observe a small distribution
of signal values. This is due outliers in the signal data set.
\\
In figure \ref{fig:ATraining}, I plotted a similar plot as described above, but using a trained model.
In this figure the output is far more separated, and we see noticeable differences in the activation 
of nodes. To highlight the difference in activation, I drew two new figures where the signal 
(\ref{fig:ATrainingSig}) and background (\ref{fig:ATrainingBkg}) were drawn individually.
In the two figures we notice that there is still a noticeable variation in the activation for
both signal and background. This is due to outliers in the data, but also the variation of trend.
Regardless, there are still clear trends in activation which lets us know that the model has found 
specific paths in the network for signal and background separately. Most noticeably, the two group 
in the middle hidden layer highlight this fact. In the case of the signal, the upper group show a clear 
favoritism to the second bottom node. For the background this is also partly true, but with far more 
spread in the other nods. Similarly, in the bottom group (in the same layer), the background shows large
activation in the uppermost node, while the signal data does not. 
\begin{figure}
    \makebox[\linewidth][c]{%
    \centering
    \begin{subfigure}{.6\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/NetworkVis/BeforeTraining.pdf}
        \caption{}
        \label{fig:BTraining}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.6\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/NetworkVis/AfterTraining.pdf}
        \caption{}
        \label{fig:ATraining}
    \end{subfigure}
    }
    \caption{A calculated visualization of a 3 layer MaxOut network. Each path 
    represents a data point where all connected nodes were the largest activation in their respective 
    group. The distribution on the far right represent the output distribution. The figure to the left
    (\ref{fig:ATrainingSig}) is the result before training and the figure to right (\ref{fig:ATrainingBkg})
    is after.}. 
\end{figure}


\begin{figure}
    \makebox[\linewidth][c]{%
    \centering
    \begin{subfigure}{.6\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/NetworkVis/AfterTrainingSig.pdf}
        \caption{}
        \label{fig:ATrainingSig}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.6\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/NetworkVis/AfterTrainingBkg.pdf}
        \caption{}
        \label{fig:ATrainingBkg}
    \end{subfigure}
    }
    \caption{A calculated visualization of a 3 layer MaxOut network. Each path 
    represents a data point where all connected nodes were the largest activation in their respective 
    group. The distribution on the far right represent the output distribution and the figure with blue 
    paths \ref{fig:ATrainingSig} is the result of signal and the figure pink \ref{fig:ATrainingBkg}.
    } 
    \label{fig:NetDist1}
\end{figure}

\begin{figure}
    \makebox[\linewidth][c]{%
    \centering
    \begin{subfigure}{.6\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/NetworkVis/AfterTrainingSig50250.pdf}
        \caption{}
        \label{fig:ATrainingSig50250}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.6\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/NetworkVis/AfterTrainingSig200300.pdf}
        \caption{}
        \label{fig:ATrainingSig200300}
    \end{subfigure}
    }
    \caption{A calculated visualization of a trained MaxOut network with 3 hidden layers. Each path 
    represents a data point where all connected nodes were the largest activation in their respective 
    group. The figure to the left (\ref{fig:ATrainingSig50250}) is a result of signal with 
    ($\tilde{\chi}_1=50$, $\tilde{\chi}_2=250$GeV) and the right (\ref{fig:ATrainingSig200300}) 
    ($\tilde{\chi}_1=200$, $\tilde{\chi}_2=300$GeV).}
    \label{fig:NetVisSigComp}
\end{figure}

\begin{figure}
    \makebox[\linewidth][c]{%
    \centering
    \hfill
    \hfill
    \begin{subfigure}{.1\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/NetworkVis/AfterTrainingSig50250Zoom.pdf}
        \caption{}
        \label{fig:ATrainingSig50250Zoom}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.104\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/NetworkVis/AfterTrainingSig200300Zoom.pdf}
        \caption{}
        \label{fig:ATrainingSig200300Zoom}
    \end{subfigure}
    \hfill
    \hfill
    }
    \caption{A cut-out of the fifth and sixth neuron (counting from the top) in the second hidden layer, activated 
    by the signal with ($\tilde{\chi}_1=50$, $\tilde{\chi}_2=250$GeV) \ref{fig:ATrainingSig50250Zoom} and 
    ($\tilde{\chi}_1=200$, $\tilde{\chi}_2=300$GeV) \ref{fig:ATrainingSig200300Zoom}.}
    \label{fig:NetVisZoom}
\end{figure}
\subsection{Training History and Overfitting}
In section \ref{sec:Regularization} I described how creating ensembles of networks is a form of regularization. Therefore,
it is of interest to study the relationship between performance on the training set and the validation set for our ensemble 
methods. In figure \ref{fig:History} I drew plots displaying the performance on the training and validation scores after each 
epoch, as measured in \ref{AUC} for both a dense \ac{NN} (\ref{fig:NNHist}) and maxout (\ref{fig:MaxOutHist}).
\\ 
In figure \ref{fig:NNHist} we can observe that a deep, dense \ac{NN} reaches a maximum in performance for the validation set after only 
a couple epochs. This peak is then followed by a quick drop in performance, while the training set increases in performance. 
The drop in performance for the validation set and increase in performance for the training set is a sure sign of overfitting. 
\\
In figure \ref{fig:MaxOutHist} we observe that an ensemble method (in this case the maxout) displays a different trend. For the first 
10 epochs the model increases in performance for both data sets. After this, the validation set does not decrease in performance, but 
simply holds stable while the training set increases. This is a sign that the model is not experiencing overfitting.
\\
In section \ref{subsec:TrainingStrategy}, I discussed the training strategy used when training all models in this analysis. I mentioned 
the implementation of an early-stopping criteria which makes sure that the model only continues to train as long as the performance on the 
validation data set increases. By studying the subfigures in figure \ref{fig:History}, we can deduce that the ensemble methods will not only 
be able to avoid overfitting, but will as a consequence be allowed to train much longer than the other models.

\begin{figure}
    \makebox[\linewidth][c]{%
    \centering
    \begin{subfigure}{.45\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/SUSY/History/NNHistory.pdf}
        \caption{}
        \label{fig:NNHist}
    \end{subfigure}
    \begin{subfigure}{.45\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/SUSY/History/MaxOutHistory.pdf}
        \caption{}
        \label{fig:MaxOutHist}
    \end{subfigure}
    }
    \caption{A plot displaying the \ac{AUC} score made after each epoch on both the training and validation set. 
    Figure \ref{fig:NNHist} shows the results from the dense \ac{NN} and figure \ref{fig:MaxOutHist} shows
    the results from a maxout network.}
    \label{fig:History}
\end{figure}
\subsection{Sensitivity Result}
\begin{figure}
    \makebox[\linewidth][c]{%
    \centering
    \begin{subfigure}{.65\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/SUSY/Grid/MaxOutGridSig.pdf}
    \end{subfigure}
    }
    \caption{A grid displaying the achieved significance on the original signal set, using the signal region 
    created by the \emph{MaxOut} network.}
    \label{fig:MaxOutGridSig}
\end{figure}

\begin{figure}
    \makebox[\linewidth][c]{%
    \centering
    \begin{subfigure}{.75\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/SUSY/Comparison/EnsemblesNetworkComp.pdf}
    \end{subfigure}
    }
    \caption{A sensitivity comparison between the ensemble methods (maxout, \ac{SCO}, channel-Out) on the original 
    signal data. The size of the "pie" represents the relative size of the significance and the color around each 
    point displays the method with the largest sensitivity for the respective combination.}
    \label{fig:EnsembleComp}
\end{figure}

