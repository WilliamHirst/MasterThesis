\section{Dense Neural Networks}
\subsection{Deep vs Shallow}
In previous sections, I have discussed the reasoning for not wanting an automated process for building network architectures (otherwise known
as hyperparameter searches). Although, I found manually choosing the architecture for each model to be preferable in this analysis, there 
are some downsides to that decision. When building each model I wanted to ensure that the performance of said models was representative of 
the category of models I wanted to build. In other words, I wanted to ensure that any drawbacks were related to the type of model and not 
a result of poorly chosen number of layers or nodes. Therefore, I wanted to relieve as much randomness in the choice of architecture as possible.
\\
In this section I present a comparison between the results from two dense \ac{NN}, one deep and one shallow. The deep network 
uses the dense \ac{NN} architecture described in \ref{subsec:arch}, while the shallow network uses the same amount of hidden layers,
but with only 20 nodes in each. The comparison represents a sample of networks I choose to compare when deciding the number of nodes and 
hidden layers would be used in the analysis. Both networks, deep and shallow utilized the training strategy described in section 
\ref{subsec:TrainingStrategy}. In figure \ref{fig:NNshallowGridSig}, I drew a grid displaying the achieved sensitivity for the shallow 
network. By comparing with the results achieved by the XGBoost model in figure \ref{fig:XGBoost}, we can observe that the shallow network 
performs very similarly to the XGBoost model. In fact, the XGBoost model seems to outperform the shallow network ever so slightly.
\\
In figure \ref{fig:NNGridSig}, I present the same grid as described above, but using the deep network. By comparing the results from the shallow 
network in figure \ref{fig:NNshallowGridSig}, we can discern that the deep network outperforms the shallow network for every single mass 
combination in the original signal set. This result motivated the choice of layers and nodes for all networks used in the sections to come.
\begin{figure}
    \makebox[\linewidth][c]{%
    \centering
    \begin{subfigure}{.65\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/SUSY/Grid/NNshallowGridSig.pdf}
    \end{subfigure}
    }
    \caption{A grid displaying the achieved significance on the original signal set, using the signal region 
    created by the shallow \ac{NN}.}
    \label{fig:NNshallowGridSig}
\end{figure}
\begin{figure}
    \makebox[\linewidth][c]{%
    \centering
    \begin{subfigure}{.65\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/SUSY/Grid/NNGridSig.pdf}
    \end{subfigure}
    }
    \caption{A grid displaying the achieved significance on the original signal set, using the signal region 
    created by the dense \ac{NN}.}
    \label{fig:NNGridSig}
\end{figure}
\subsection{Parameter Specific Networks and Interpolation}
As I have touched upon in earlier sections, one possible solution to a diverse signal set (in my case a signal set with many 
potential mass combination) is to implement one model for each individual signal set. Initially one might assume that although 
this approach demands more work, it also produces the strongest performances for all mass combinations individually. I have already 
discussed how by including different mass combinations in the same signal, we hope to reduce overfitting and potentially allow 
the model to interpolate between the different mass combinations. However, I have not yet discussed how by moving from a one 
mass combination signal set to a diverse set would affect the performance on the original signal. In this section I will present the 
results from an analysis which aims to test the affirmation affect.
\\
In figure \ref{fig:Interpolation}, I present the results from two models who have each trained on two different amounts of signal.
The training used to produce the results aligns with the strategy described in section \ref{subsec:TrainingStrategy}.
Figure \ref{fig:OneMass} presents the achieved sensitivity from a dense \ac{NN} which has only trained on one mass combination,
($\tilde{\chi}_1=250$, $\tilde{\chi}_2=550$GeV), while figure \ref{fig:SeveralMass} presents the achieved sensitivity (using the same 
architecture) after training on a large set of mass combinations. Specifically, the latter figure shows the results after training on 
all signals on the outer square in the figure \ref{fig:SeveralMass} and ($\tilde{\chi}_1=250$, $\tilde{\chi}_2=550$GeV).\\
\begin{figure}
    \makebox[\linewidth][c]{%
    \centering
    \begin{subfigure}{.6\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/SUSY/Grid/Interpolation/NN_OneMass_InterpolationGridSig.pdf}
        \vspace{-1.cm}
        \caption{}
        \label{fig:OneMass}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.6\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/SUSY/Grid/Interpolation/NN_InterpolationGridSig.pdf}
        \vspace{-1.cm}
        \caption{}
        \label{fig:SeveralMass}
    \end{subfigure}
    }
    \caption{Two grids displaying the achieved significance on a subset of the full signal set, using the signal region 
    created by the \ac{NN}. Figure \ref{fig:OneMass} presents the results from a model who has only seen one mass combination 
    during training, ($\tilde{\chi}_1=250$, $\tilde{\chi}_2=550$GeV). Figure \ref{fig:SeveralMass} presents the results from 
    a model who has seen all mass combinations in the grid, but for the inner square of masses. }
    \label{fig:Interpolation}
\end{figure}
My initial prediction of this comparison, was that the one-mass model would outperform the several-masses model for the signal with
($\tilde{\chi}_1=250$, $\tilde{\chi}_2=550$GeV), while underperforming on all other data points. The expectation was that one-mass model 
would learn the trends of the only signal it had seen, while the several-masses model would do the same but for more mass combination and 
at the same time interpolate the results for the masses in between. By comparing figures \ref{fig:OneMass} and \ref{fig:SeveralMass} we 
see that my prediction was not completely true. We can observe that the model which has trained on several-masses outperformed the 
one-mass model on every single signal. At first, I believed this to be caused by the one-mass model overfitting a lot sooner than the other 
model, therefore being stopped earlier in training by the early-stopping criteria described in section \ref{subsec:TrainingStrategy}. 
To test this theory, I drew the training history of each training session.
\\
In figure \ref{fig:InterpolateHistory} I drew the \ac{AUC} score made after each epoch on both the training and validation set
for the one-mass model (\ref{fig:oneMassHist}) and the several-masses model \ref{fig:SeveralMassHist}. By comparing the two figures,
we observe that the one-mass models performance on the validation set peaks in the first epoch, therefore stopping training after 
10 epochs, while the several-masses model peaks after 6 epochs. This is a clear indication that training on one mass combination, leads 
to overfitting. 
\begin{figure}
    \makebox[\linewidth][c]{%
    \centering
    \begin{subfigure}{.45\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/SUSY/History/NN_OneMass_InterpolationHistory.pdf}
        \caption{}
        \label{fig:oneMassHist}
    \end{subfigure}
    \begin{subfigure}{.45\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/SUSY/History/NN_InterpolationHistory.pdf}
        \caption{}
        \label{fig:SeveralMassHist}
    \end{subfigure}
    }
    \caption{A plot displaying the \ac{AUC} score made after each epoch on both the training and validation set. 
    Figure \ref{fig:oneMassHist} shows the results from the one-mass model and figure \ref{fig:SeveralMassHist} shows
    the results from the several-masses model.}
    \label{fig:InterpolateHistory}
\end{figure}
\\
Given the one-mass models performance is reduced by the early-stopping criteria, I wanted to explore if it would be able to outperform the 
several-masses model given it was allowed to train deeper. In figure \ref{fig:NNOverfitting}, I display the achieved significance
of the one-mass model in the case where early-stopping has been removed, and the model was allowed to train for 15 epochs. By comparing figures 
\ref{fig:SeveralMass} and \ref{fig:NNOverfitting}, we can observe that even when early-stopping is removed from the one-mass model, it is  
still outperformed by the several-masses model. This indicates that overfitting is not the only reason for the one-mass models disappointing performance.\\
\begin{figure}
    \centering
    \makebox[\linewidth][c]{%
    \begin{subfigure}{.6\textwidth}
        \includegraphics[width=\textwidth]{Figures/MLResults/NN/SUSY/Grid/Interpolation/NN_OneMass_Overfitting15_InterpolationGridSig.pdf}
    \end{subfigure}
    }
    \caption{A grid displaying the achieved significance on a subset of the full signal set, using the signal region 
    created by the \ac{NN}. The figure presents the results from a model who has only seen one mass combination 
    during training, ($\tilde{\chi}_1=250$, $\tilde{\chi}_2=550$GeV) and was allowed to train for 15 epochs without 
    early-stopping.}
    \label{fig:NNOverfitting}
\end{figure}
The most probable explanation for the underwhelming performance of the one-mass model, is that nearby mass combinations (i.e. mass combination with 
relatively similar masses) exhibit a lot of overlap in terms of feature trends. In other words, nearby mass combinations (in this example, mass combinations 
differ by 100GeV) often contribute to the same type of training. This means that by including a larger range of signals, which are similar in mass, we are 
essentially increasing the amount of data for each individual signal.