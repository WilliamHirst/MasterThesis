\acswitchoff 
\babel@toc {UKenglish}{}\relax 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Event Cross Section in a computer generated image of the ATLAS detector \cite {PDetector}.\relax }}{7}{figure.caption.9}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces An illustration of the general kinematics in a particle-collision, inspired by the figure in the paper by Gramstad \cite {gramstad_searches_nodate}. The illustration shows both the longitudial and transverse plane. \relax }}{8}{figure.caption.10}%
\contentsline {figure}{\numberline {1.3}{\ignorespaces Feynman diagrams of background processes.}}{11}{figure.caption.11}%
\contentsline {figure}{\numberline {1.4}{\ignorespaces The Feynman diagram of the signal-channel.\relax }}{12}{figure.caption.12}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces The distribution of the two PCA-features containing most variation for (left to right, up to down) 10, 10, 100 and 300 samples from each channel. Each sample filling the requirement with being less than one standard deviation from the mean of both features, respectively.\relax }}{17}{figure.caption.15}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces The distribution of the two PCA-features containing the least amount of variation for (left to right, up to down) 10, 10, 100 and 300 samples from each channel. Each sample filling the requirement with being less than one standard deviation from the mean of both features, respectively.\relax }}{17}{figure.caption.16}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces An illustration of a \ac {NN} with two hidden layers.\relax }}{19}{figure.caption.17}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces An illustration information pass for one layer to another.\relax }}{20}{figure.caption.18}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces An illustration of a Neural network with two hidden layers, each with 8 neurons. The first hidden layer has a maxout activation layer with 4 units, the second has 2. The figure also illustrates the resulting ensemble of smaller neural networks as a consequence of the maxout activation layers. \relax }}{23}{figure.caption.23}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces An illustration of three different layers, channelout, \ac {SCO} and maxout. The figure shows how each layer redimensionalize the network and how channelout and \ac {SCO} differ from maxout in regard to the relationship between units and parameters.\relax }}{24}{figure.caption.24}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces An illustration of a comparison between the parameter individualistic network approach and the \ac {PNN}.\relax }}{24}{figure.caption.25}%
\contentsline {figure}{\numberline {2.8}{\ignorespaces An illustration of a simple \ac {DT}, mapping a 4 dimensional input to one of three values in the target space.\relax }}{25}{figure.caption.26}%
\contentsline {figure}{\numberline {2.9}{\ignorespaces An illustration of a traditional cut-and-count approach as well as the motivation for creating a \ac {ML} variable.\relax }}{27}{figure.caption.27}%
\contentsline {figure}{\numberline {2.10}{\ignorespaces An illustration of a ROC curve.\relax }}{28}{figure.caption.28}%
\contentsline {figure}{\numberline {2.11}{\ignorespaces An illustration of a Gaussian distibution and the area under the curve defined by a significance equal to 1.64.\relax }}{29}{figure.caption.29}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces A depiction of all mass combinations in the full signal data set.\relax }}{32}{figure.caption.30}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces A visual summary of the workflow and framework use for the computational analysis. \relax }}{33}{figure.caption.31}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Two tables displaying the baseline \ref {table:BL} and signal \ref {table:SG} requirements applied to the data as part of the preprocessing.\relax }}{36}{figure.caption.32}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces References to figures for all lepton (\ref {table:Ref3L}) and event (\ref {table:RefGen}) specific feature distribution.\relax }}{36}{figure.caption.34}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces \ac {MC} and real data comparison and event distribution for each channel over $P_t$ \ref {fig:lep1_Pt} and $\eta $ \ref {fig:lep1_Eta} for the first lepton. Similarly, the distribution over the $E_T^{miss}$ \ref {fig:met_Et} and flavor combination of the three leptons \ref {fig:flcomp}.\relax }}{37}{figure.caption.35}%
\contentsline {figure}{\numberline {3.6}{\ignorespaces The event distributions of the leading lepton for the features $P_t$ and $\phi $. Figures \ref {fig:lep1_Pt_Neg} and \ref {fig:lep1_Phi_Neg} display only the events with negative weights (for $P_t$ and $\phi $ respectively) whereas \ref {fig:lep1_Pt_nNeg} and \ref {fig:lep1_Phi_nNeg} show the full data set.\relax }}{38}{figure.caption.36}%
\contentsline {figure}{\numberline {3.7}{\ignorespaces A visual summary of the workflow and framework use for the computational analysis. \relax }}{42}{figure.caption.40}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces A grid displaying the achieved significance on the original signal set, using the signal region created by the \emph {XGBoost} network.\relax }}{46}{figure.caption.44}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces The output distribution from a trained XGBoost model for the background and signals with 4 different mass combinations: ($\tilde {\chi }_1=200$, $\tilde {\chi }_2=400$GeV), ($\tilde {\chi }_1=400$, $\tilde {\chi }_2=650$GeV), ($\tilde {\chi }_1=700$, $\tilde {\chi }_2=50$GeV) and ($\tilde {\chi }_1=400$, $\tilde {\chi }_2=800$GeV). The figure includes the full output range (\ref {fig:XGBDist}) and the output ranging from 0.9-1.00 (\ref {fig:XGBDist_95}).\relax }}{46}{figure.caption.45}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces A grid displaying the achieved significance on the original signal set, using the signal region created by the shallow \ac {NN}.\relax }}{47}{figure.caption.46}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces A grid displaying the achieved significance on the original signal set, using the signal region created by the dense \ac {NN}.\relax }}{48}{figure.caption.47}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces Two grids displaying the achieved significance on a subset of the full signal set, using the signal region created by the \ac {NN}. Figure \ref {fig:OneMass} presents the results from a model who has only seen one mass combination during training, ($\tilde {\chi }_1=250$, $\tilde {\chi }_2=550$GeV). Figure \ref {fig:SeveralMass} presents the results from a model who has seen all mass combinations in the grid, but for the inner square of masses. \relax }}{49}{figure.caption.48}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces A plot displaying the \ac {AUC} score made after each epoch on both the training and validation set. Figure \ref {fig:oneMassHist} shows the results from the one-mass model and figure \ref {fig:SeveralMassHist} shows the results from the several-masses model.\relax }}{49}{figure.caption.49}%
\contentsline {figure}{\numberline {4.7}{\ignorespaces A grid displaying the achieved significance on a subset of the full signal set, using the signal region created by the \ac {NN}. The figure presents the results from a model who has only seen one mass combination during training, ($\tilde {\chi }_1=250$, $\tilde {\chi }_2=550$GeV) and was allowed to train for 15 epochs without early-stopping.\relax }}{50}{figure.caption.50}%
\contentsline {figure}{\numberline {4.8}{\ignorespaces A calculated visualization of a 3 layer MaxOut network. Each path represents a data point where all connected nodes were the largest activation in their respective unit. The distribution on the far right represent the output distribution. The figure to the left (\ref {fig:ATrainingSig}) is the result before training and the figure to right (\ref {fig:ATrainingBkg}) is after.\relax }}{51}{figure.caption.51}%
\contentsline {figure}{\numberline {4.9}{\ignorespaces A calculated visualization of a 3 layer MaxOut network. Each path represents a data point where all connected nodes were the largest activation in their respective unit. The distribution on the far right represent the output distribution and the figure with blue paths \ref {fig:ATrainingSig} is the result of signal and the figure pink \ref {fig:ATrainingBkg}. \relax }}{52}{figure.caption.52}%
\contentsline {figure}{\numberline {4.10}{\ignorespaces A calculated visualization of a trained MaxOut network with 3 hidden layers. Each path represents a data point where all connected nodes were the largest activation in their respective unit. The figure to the left (\ref {fig:ATrainingSig50250}) is a result of signal with ($\tilde {\chi }_1=50$, $\tilde {\chi }_2=250$GeV) and the right (\ref {fig:ATrainingSig200300}) ($\tilde {\chi }_1=200$, $\tilde {\chi }_2=300$GeV).\relax }}{52}{figure.caption.53}%
\contentsline {figure}{\numberline {4.11}{\ignorespaces A cut-out of the fifth and sixth node (counting from the top) in the second hidden layer, activated by the signal with ($\tilde {\chi }_1=50$, $\tilde {\chi }_2=250$GeV) \ref {fig:ATrainingSig50250Zoom} and ($\tilde {\chi }_1=200$, $\tilde {\chi }_2=300$GeV) \ref {fig:ATrainingSig200300Zoom}.\relax }}{52}{figure.caption.54}%
\contentsline {figure}{\numberline {4.12}{\ignorespaces A plot displaying the \ac {AUC} score made after each epoch on both the training and validation set. Figure \ref {fig:NNHist} shows the results from the dense \ac {NN} and figure \ref {fig:MaxOutHist} shows the results from a maxout network.\relax }}{53}{figure.caption.55}%
\contentsline {figure}{\numberline {4.13}{\ignorespaces A grid displaying the achieved significance on the original signal set, using the signal region created by the \emph {MaxOut} network.\relax }}{54}{figure.caption.56}%
\contentsline {figure}{\numberline {4.14}{\ignorespaces A sensitivity comparison between the ensemble methods (maxout, \ac {SCO}, channel-Out) on the original signal data. The size of the "pie" represents the relative size of the significance and the color around each point displays the method with the largest sensitivity for the respective combination.\relax }}{55}{figure.caption.57}%
\contentsline {figure}{\numberline {4.15}{\ignorespaces The output distribution from a trained \ac {PNN} model for the background and signals with 4 different mass combinations: ($\tilde {\chi }_1=50$, $\tilde {\chi }_2=250$GeV), ($\tilde {\chi }_1=100$, $\tilde {\chi }_2=200$GeV), ($\tilde {\chi }_1=200$, $\tilde {\chi }_2=300$GeV) and ($\tilde {\chi }_1=150$, $\tilde {\chi }_2=250$GeV), and where all the data were given the parameter features of ($\tilde {\chi }_1=50$, $\tilde {\chi }_2=250$GeV). The figure includes the full output range (\ref {fig:PNN50250Dist}) and the output ranging from 0.975-1.00 (\ref {fig:PNN50250Dist_95}).\relax }}{55}{figure.caption.58}%
\contentsline {figure}{\numberline {4.16}{\ignorespaces The output distribution from a trained \ac {PNN} model for the background and signals with 4 different mass combinations: ($\tilde {\chi }_1=50$, $\tilde {\chi }_2=250$GeV), ($\tilde {\chi }_1=100$, $\tilde {\chi }_2=200$GeV), ($\tilde {\chi }_1=200$, $\tilde {\chi }_2=300$GeV) and ($\tilde {\chi }_1=150$, $\tilde {\chi }_2=250$GeV), and where all the data were given the parameter features of ($\tilde {\chi }_1=200$, $\tilde {\chi }_2=300$GeV). The figure includes the full output range (\ref {fig:PNN50250Dist}) and the output ranging from 0.975-1.00 (\ref {fig:PNN50250Dist_95}).\relax }}{56}{figure.caption.59}%
\contentsline {figure}{\numberline {4.17}{\ignorespaces A grid displaying the achieved significance on the original signal set, using the signal region created by the \ac {PNN} network.\relax }}{57}{figure.caption.61}%
\contentsline {figure}{\numberline {4.18}{\ignorespaces A sensitivity comparison between a dense \ac {NN}, \ac {PNN}, maxout and XGBoost on the original signal data. The size of the "pie" represents the relative size of the significance and the color around each point displays the method with the largest sensitivity for the respective combination.\relax }}{58}{figure.caption.62}%
\contentsline {figure}{\numberline {4.19}{\ignorespaces Two pie-plots comparing sensitivity on the original signal set, where each figure shows the comparison between a model training on data with and without a \ac {PCA}. Figure \ref {fig:MaxOutPCAComp} displays the comparison for the maxout model, and likewise figure \ref {fig:PNNPCAComp} for the \ac {PNN}. The size of the "pie" represents the relative size of the significance and the color around each point displays the method with the largest sensitivity for the respective combination.\relax }}{59}{figure.caption.63}%
\contentsline {figure}{\numberline {4.20}{\ignorespaces A grid displaying the achieved significance on the original signal set, using the signal region created by the \ac {NN} network. A \ac {PCA} analysis has been applied to the data being utilized in this result.\relax }}{60}{figure.caption.64}%
\contentsline {figure}{\numberline {4.21}{\ignorespaces A grid displaying the achieved significance on the full statistics signal set, using the signal region created by the \ac {NN} network.\relax }}{61}{figure.caption.65}%
\contentsline {figure}{\numberline {4.22}{\ignorespaces A grid displaying the achieved significance on the full statistics signal set, using the signal region created by the \emph {MaxOut} network.\relax }}{61}{figure.caption.66}%
\contentsline {figure}{\numberline {4.23}{\ignorespaces A grid displaying the achieved significance on the full statistics signal set, using the signal region created by the \emph {PNN} network.\relax }}{62}{figure.caption.67}%
\contentsline {figure}{\numberline {4.24}{\ignorespaces A grid displaying the achieved significance on the full statistics signal set, using the signal region created by the \emph {PNN} network.\relax }}{62}{figure.caption.68}%
\contentsline {figure}{\numberline {4.25}{\ignorespaces A surface plot of the significance achieved by the \ac {PNN} on the full statistics signal set. Contours are drawn around the band equal to a significance of 1.64 for the \ac {PNN} (yellow), dense \ac {NN} (cyan), maxout model (green) and the ATLAS analysis (pink). The significance achieved by the \ac {ML} models were calculated with a flat uncertainty equal to $20\%$ of the background.\relax }}{63}{figure.caption.69}%
\contentsline {figure}{\numberline {4.26}{\ignorespaces Two surface plots of the significance achieved by the \ac {PNN} on the full statistics signal set. Contours are drawn around the band equal to a significance of 1.64 for the \ac {PNN} (yellow), dense \ac {NN} (cyan), maxout model (green) and the ATLAS analysis (pink). The significance achieved by the \ac {ML} models were calculated with a flat uncertainty equal to $10\%$ of the background in the left figure \ref {fig:compLimit10} and less than $1\%$ in the right \ref {fig:compLimit1}.\relax }}{64}{figure.caption.70}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {27}{\ignorespaces A grid displaying the achieved significance on the original signal set, using the signal region created by the \ac {SCO} \ref {fig:StochChannelOutGridSig} and a channel-out network \ref {fig:ChannelOutGridSig}.\relax }}{72}{figure.caption.75}%
\contentsline {figure}{\numberline {28}{\ignorespaces A grid displaying the achieved significance on the original signal set, using the signal region created by the \ac {NN} \ref {fig:NNPCAGridSig} and a maxout network \ref {fig:MaxOutPCAGridSig}. A \ac {PCA} analysis has been applied to the data being utilized in this result.\relax }}{72}{figure.caption.76}%
\contentsline {figure}{\numberline {29}{\ignorespaces A grid displaying the achieved significance on the original signal set, using the signal region created by the \ac {PNN} network. A \ac {PCA} analysis has been applied to the data being utilized in this result.\relax }}{73}{figure.caption.77}%
\contentsline {figure}{\numberline {30}{\ignorespaces A grid displaying the achieved significance on the original signal set, using the signal region created by the \ac {NN} network. A \ac {PCA} analysis has been applied to the data being utilized in this result.\relax }}{73}{figure.caption.78}%
\contentsline {figure}{\numberline {31}{\ignorespaces \ac {MC} and real data comparison and event distribution for each channel over $P_t$ for the first \ref {fig:lep1_Pt}, second \ref {fig:lep2_Pt} and third \ref {fig:lep3_Pt} lepton. Similarly, the distribution over $\eta $ for the \ref {fig:lep1_Eta}, second \ref {fig:lep2_Eta} and third \ref {fig:lep3_Eta} lepton.\relax }}{74}{figure.caption.79}%
\contentsline {figure}{\numberline {32}{\ignorespaces \ac {MC} and real data comparison and event distribution for each channel over $\phi $ for the first \ref {fig:lep1_Phi}, second \ref {fig:lep2_Phi} and third \ref {fig:lep3_Phi} lepton. Similarly, the distribution over $m_t$ for the first \ref {fig:lep1_Mt}, second \ref {fig:lep2_Mt} and third \ref {fig:lep3_Mt} lepton.\relax }}{75}{figure.caption.80}%
\contentsline {figure}{\numberline {33}{\ignorespaces \ac {MC} and real data comparison and event distribution for each channel over the charge for the first \ref {fig:lep1_Charge} , second \ref {fig:lep2_Charge} and third \ref {fig:lep3_Charge} lepton. Similarly, the distribution over the flavor for the first \ref {fig:lep1_Flavor}, second \ref {fig:lep2_Flavor} and third \ref {fig:lep3_Flavor} lepton\relax }}{76}{figure.caption.81}%
\contentsline {figure}{\numberline {34}{\ignorespaces \ac {MC} and real data comparison and event distribution for for each channel over the energy \ref {fig:met_Et} and azimuthal angel \ref {fig:met_Phi} for the transverse momentum. The distribution of the invariant mass of the three leptons \ref {fig:mlll} and the OSSF pair \ref {fig:mll_OSSF}. The distribution over the significance of the missing transverse enegy \ref {fig:met_Sign} and the sum of $P_t$ \ref {fig:Ht_lll}.\relax }}{77}{figure.caption.82}%
\contentsline {figure}{\numberline {35}{\ignorespaces \ac {MC} and real data comparison and event distribution for each channel over the sum of $P_t$ for the SS pair \ref {fig:Ht_SS} and the sum over all three leptons added with $E_t^{miss}$ \ref {fig:Ht_met_Et}. The distribution over $\Delta R$ \ref {fig:deltaR} and the flavor combination of the three leptons \ref {fig:flcomp}. The distribution of number of jets \ref {fig:njet_SG} and the mass of the leading di-jet pair \ref {fig:M_jj}.\relax }}{78}{figure.caption.83}%
