% https://github.com/martinhelso/MathDept


\documentclass[UKenglish]{beamer}


\usetheme[NoLogo]{MathDept}


\usepackage[utf8]{inputenx} % For æ, ø, å
\usepackage{babel}          % Automatic translations
\usepackage{csquotes}       % Quotation marks
\usepackage{microtype}      % Improved typography
\usepackage{amssymb}        % Mathematical symbols
\usepackage{mathtools}      % Mathematical symbols
\usepackage[absolute, overlay]{textpos} % Arbitrary placement
\setlength{\TPHorizModule}{\paperwidth} % Textpos units
\setlength{\TPVertModule}{\paperheight} % Textpos units
\usepackage{tikz}
\usetikzlibrary{overlay-beamer-styles}  % Overlay effects for TikZ


\author{William Hirst}
\title[Supervised Learning in HEP]{Application of Supervised Machine Learning to the Search for New Physics in ATLAS data}
\subtitle{A Study of Ordinary Dense, Parameterized and Ensemble Networks and their Application to High Energy Physics}


\begin{document}


\section{Overview}

\begin{frame}{Outline}
    \tableofcontents
\end{frame}

\SubSectionPage{Overview}

\section{Introduction $\&$ Motivation}
\begin{frame}{Outline}
    \tableofcontents[currentsection]
\end{frame}

\subsection{Why apply machine learning to HEP problems?}
\begin{frame}{Why apply machine learning to HEP problems?}
 
\end{frame}


\subsection{How do we search for new physics?}
\begin{frame}{How do we search for new physics?}
 
\end{frame}

\section{The Implementation}
\begin{frame}{Outline}
    \tableofcontents[currentsection]
\end{frame}

\subsection{A summary of the applied methods}
\begin{frame}{A summary of the applied methods}
    \emph{Three} neural network variants
    \begin{itemize}
        \item Ordinary dense neural network
        \item Ensemble networks utilizing Local-Winner-Takes-All (LWTA) layers
        \item Parameterized neural networks (PNN)
    \end{itemize}
    \emph{One} boosted decision tree method
    \begin{itemize}
        \item XGBoost using default settings
    \end{itemize}
\end{frame}

\subsection{How are the methods compared?}
\begin{frame}{How are the methods compared?}

\end{frame}

\subsection{Training strategy}
\begin{frame}{Training strategy}
 
\end{frame}


\section{Methods $\&$ Results}
\begin{frame}{Outline}
    \tableofcontents[currentsection]
\end{frame}

\subsection*{An introduction and study of each method}
\SubSectionPage{An introduction and study of each method}


\begin{frame}{Ordinary dense neural network}
    
\end{frame}


\begin{frame}{Ensemble methods - LWTA}
    \begin{itemize}
        \item Dropout
        \item What is LWTA?
        \item Competing nodes - Units
        \item Encode information in pattern specific pathways
    \end{itemize}    
    \begin{textblock}{0.58}(0.2, 0.365)
        \includegraphics[width = \textwidth]{figures/Max_out}
    \end{textblock}
\end{frame}

\begin{frame}{Channel-Out, SCO and Maxout}
    \vspace{0.5cm}
    \center
        \begin{table}
            $
            \begin{array}{cccc}
                \hline \text { Layer } & \text { Separate Weights $\&$ Biases } & \text { Static Units }\\
                \hline\hline\text{Channel-Out} & \textcolor{green}{Yes} &  \textcolor{green}{Yes}  \\
                \text { SCO } &  \textcolor{green}{Yes} &  \textcolor{red}{No}  \\
                \text{ Maxout } &  \textcolor{red}{No} &  \textcolor{green}{Yes}   \\
                \hline
            \end{array}
            $
        \end{table}
    \begin{textblock}{0.8}(0.1, 0.45)
        \includegraphics[width = \textwidth]{figures/EnsembleComp}
    \end{textblock}
\end{frame}

\begin{frame}{Visualization and study of sparse pathways}
    \begin{itemize}
        \item A study of the implementation \\
        and effect of LWTA layers
        \item Visualize the activation and \\
        paths of 100 randomly sampled \\
        events
        \begin{itemize}
            \item 50 background
            \item 50 signal
        \end{itemize}
        \item The bolder the line the more\\ 
        frequently the path is used.
    \end{itemize}
\end{frame}
\begin{frame}
    \begin{textblock}{0.8}(0.015, 0.25)
        \textbf{Before training}
    \end{textblock}
    \begin{textblock}{0.8}(0.017, 0.7)
        \textbf{After training}
    \end{textblock}
    \begin{center}\vspace{-0.5cm}
        \includegraphics[width = 0.5\textwidth]{figures/NetworkVis/BeforeTraining.pdf}
        \includegraphics[width = 0.5\textwidth]{figures/NetworkVis/AfterTraining.pdf}
    \end{center}
\end{frame}
\begin{frame}
    \begin{textblock}{0.8}(0.015, 0.25)
        \textbf{Background}
    \end{textblock}
    \begin{textblock}{0.8}(0.017, 0.7)
        \textbf{Signal}
    \end{textblock}
    \begin{center}\vspace{-0.5cm}
        \includegraphics[width = 0.5\textwidth]{figures/NetworkVis/AfterTrainingBkg.pdf}
        \includegraphics[width = 0.5\textwidth]{figures/NetworkVis/AfterTrainingSig.pdf}
    \end{center}
\end{frame}

\begin{frame}{Comparing activation of Maxout with SCO}
    \begin{textblock}{0.8}(0.225, 0.625)
        \textbf{Maxout}
    \end{textblock}
    \begin{textblock}{0.8}(0.7, 0.625)
        \textbf{SCO}
    \end{textblock}
    \begin{center}
        \includegraphics[width = 0.475\textwidth]{figures/NetworkVis/AfterTraining.pdf}
        \includegraphics[width = 0.475\textwidth]{figures/NetworkVis/SCO/AfterTraining.pdf}
    \end{center}
\end{frame}

\begin{frame}{Parameterized neural network}
    \begin{itemize}
        \item For diverse data set, $X$, dependent on a parameter, $X(\theta)$
        \begin{itemize}
            \item Classical approach: One model for each parameter
            \item PNN approach: Include $\theta$ as feature in feature set
        \end{itemize}
        \item Signal events using masses $\{A,B\}_{GeV}$ to generate 
              event during simulation will include the parameters A and B
              in feature set
        \item Background assigned \\
              parameters randomly \\
              using same distribution\\ 
              as signal
        \item Motivation
        \begin{itemize}
            \item Network will \\
                  associate parameters\\
                  with trends in the \\
                  data
        \end{itemize}
        \begin{textblock}{0.8}(0.45, 0.45)
            \includegraphics[width = 0.7\textwidth]{figures/PNN.png}
        \end{textblock}
    \end{itemize}
\end{frame}
\begin{frame}{Study the effect of the parameters in the PNN}
    \begin{itemize}
        \item Study if the parameters effect the training as intended
        \item Test: Manually assign all the events, both background and 
        signal, the same parameters (mass combinations) thereby assigning
        most of the signal the wrong parameters
        \item Hypothesis: PNN performs better when events are assigned correct
              parameters 
        \item First test: All events are given parameters $\{50,250\}_{GeV}$
        \item Second test: All events are given parameters $\{200,300\}_{GeV}$
    \end{itemize}
\end{frame}

\begin{frame}
    \begin{center}
        \includegraphics[width = 0.475\textwidth]{figures/PNN/PNN50250Dist.pdf}
        \includegraphics[width = 0.475\textwidth]{figures/PNN/PNN50250Dist_C7.pdf}
    \end{center}
    \begin{center}
        \includegraphics[width = 0.475\textwidth]{figures/PNN/PNN200300Dist.pdf}
        \includegraphics[width = 0.475\textwidth]{figures/PNN/PNN200300Dist_C7.pdf}
    \end{center}
\end{frame}


\begin{frame}{Boosted decision trees - XGBoost}
 
\end{frame}
\begin{frame}{Comparing the sensitivity on a subset of the signal}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Comps/GenPlussXGBNetworkComp.pdf}
\end{frame}

\begin{frame}{Increasing sensitivity through a PCA}
    
\end{frame}


\begin{frame}{Comparing the methods to previous analysis}
    
\end{frame}
\section{Conclusion $\&$ Outlook}
\begin{frame}{Outline}
    \tableofcontents[currentsection]
\end{frame}




\section{References}


\begin{frame}[allowframebreaks]{References}
    \begin{thebibliography}{}

        % Article is the default.
        \setbeamertemplate{bibliography item}[book]

        \bibitem{Hartshorne1977}
        Hartshorne, R.
        \newblock \emph{Algebraic Geometry}.
        \newblock Springer-Verlag, 1977.

        \setbeamertemplate{bibliography item}[article]

        \bibitem{Helso2020}
        Helsø, M.
        \newblock \enquote{Rational quartic symmetroids}.
        \newblock \emph{Adv. Geom.}, 20(1):71--89, 2020.

        \setbeamertemplate{bibliography item}[online]

        \bibitem{HR2018}
        Helsø, M.\ and Ranestad, K.
        \newblock \emph{Rational quartic spectrahedra}, 2018.
        \newblock \url{https://arxiv.org/abs/1810.11235}

        \setbeamertemplate{bibliography item}[triangle]

        \bibitem{AM1969}
        Atiyah, M.\ and Macdonald, I.
        \newblock \emph{Introduction to commutative algebra}.
        \newblock Addison-Wesley Publishing Co., Reading, Mass.-London-Don
        Mills, Ont., 1969

        \setbeamertemplate{bibliography item}[text]

        \bibitem{Artin1966}
        Artin, M.
        \newblock \enquote{On isolated rational singularities of surfaces}.
        \newblock \emph{Amer. J. Math.}, 80(1):129--136, 1966.

    \end{thebibliography}
\end{frame}


\end{document}